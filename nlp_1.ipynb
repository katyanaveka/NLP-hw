{
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment - Transliteration"
      ],
      "metadata": {
        "id": "KslYmMKMLY7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this task you are required to solve the transliteration problem of names from English to Russian. Transliteration of a string means writing this string using the alphabet of another language with the preservation of pronunciation, although not always.\n"
      ],
      "metadata": {
        "id": "NeC1UiR2LXiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions"
      ],
      "metadata": {
        "id": "XvGHiezwIyt_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To complete the assignment please do the following  steps (both are requred to get the full credits): \n",
        "\n",
        "###1. Complete this notebook\n",
        "\n",
        "Upload a filled notebook with code (this file). You will be asked to implement a transformer-based approach for transliteration.\n",
        "\n",
        "You should implement your ``train`` and ``classify`` functions in this notebook in the cells below. Your model should be implemented as a special class/function in this notebook (be sure if you add any outer dependencies that everything is improted correctly and can be reproducable). \n",
        "\n",
        "\n",
        "###2. Submit solution to the shared task\n",
        "\n",
        "After the implementation of models' architectures you are asked to participate in the [competition](https://competitions.codalab.org/competitions/30932) to solve **Transliteration** task using your implemented code. \n",
        "\n",
        "You should use your code from the previous part to train, validate, and generate predictions for the public (Practice) and private (Evaluation) test sets. It will produce predictions (`preds_translit.tsv`) for the dataset and score them if the true answers are present. You can use these scores to evaluate your model on dev set and choose the best one. Be sure to download the [dataset](https://github.com/skoltech-nlp/filimdb_evaluation/blob/master/TRANSLIT.tar.gz) and unzip it with `wget` command and run them from notebook cells. \n",
        "\n",
        "Upload obtained TSV file with your predictions (``preds_translit.tsv``) in ``.zip`` for the best results to both phases of the competition.\n",
        "\n",
        "\n",
        "**Important: You must indicate \"DL4NLP-23\" as your team name in Codalab. Without it your submission will be invalid!**\n"
      ],
      "metadata": {
        "id": "Mop6m_5rIzu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic algorithm"
      ],
      "metadata": {
        "id": "GIr56czmR5FZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The basic algorithm is based on the following idea: for transliteration, alphabetic n-grams from one language can be transformed into another language into n-grams of the same size, using the most frequent transformation rule found according to statistics on the training sample. \n",
        "\n",
        "To test the implementation, download the data, unzip the datasets, predict transliteration and run the evaluation script. To do this, you need to run the following commands:"
      ],
      "metadata": {
        "id": "F9meQsrCR9xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/s-nlp/filimdb_evaluation/raw/master/TRANSLIT.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lf3-DST1YBx",
        "outputId": "9d9a0516-8140-44b2-a5c6-e81690e890cf",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:13:51.836273Z",
          "iopub.execute_input": "2023-04-11T19:13:51.836958Z",
          "iopub.status.idle": "2023-04-11T19:13:53.576513Z",
          "shell.execute_reply.started": "2023-04-11T19:13:51.836915Z",
          "shell.execute_reply": "2023-04-11T19:13:53.575342Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "--2023-04-11 19:13:52--  https://github.com/s-nlp/filimdb_evaluation/raw/master/TRANSLIT.tar.gz\nResolving github.com (github.com)... 192.30.255.112\nConnecting to github.com (github.com)|192.30.255.112|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/s-nlp/filimdb_evaluation/master/TRANSLIT.tar.gz [following]\n--2023-04-11 19:13:53--  https://raw.githubusercontent.com/s-nlp/filimdb_evaluation/master/TRANSLIT.tar.gz\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1546458 (1.5M) [application/octet-stream]\nSaving to: ‘TRANSLIT.tar.gz’\n\nTRANSLIT.tar.gz     100%[===================>]   1.47M  --.-KB/s    in 0.03s   \n\n2023-04-11 19:13:53 (43.2 MB/s) - ‘TRANSLIT.tar.gz’ saved [1546458/1546458]\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gunzip TRANSLIT.tar.gz"
      ],
      "metadata": {
        "id": "a1wUwZbT1lDd",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:13:53.579209Z",
          "iopub.execute_input": "2023-04-11T19:13:53.579591Z",
          "iopub.status.idle": "2023-04-11T19:13:54.544000Z",
          "shell.execute_reply.started": "2023-04-11T19:13:53.579533Z",
          "shell.execute_reply": "2023-04-11T19:13:54.542670Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xf TRANSLIT.tar"
      ],
      "metadata": {
        "id": "pg5z4ezh1zO6",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:13:54.545997Z",
          "iopub.execute_input": "2023-04-11T19:13:54.546367Z",
          "iopub.status.idle": "2023-04-11T19:13:55.495463Z",
          "shell.execute_reply.started": "2023-04-11T19:13:54.546334Z",
          "shell.execute_reply": "2023-04-11T19:13:55.494094Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline code"
      ],
      "metadata": {
        "id": "umowx2OK4IJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Any\n",
        "from random import random\n",
        "import collections as col\n",
        "\n",
        "def baseline_train(\n",
        "        train_source_strings: List[str],\n",
        "        train_target_strings: List[str]) -> Any:\n",
        "    \"\"\"\n",
        "    Trains transliretation model on the given train set represented as\n",
        "    parallel list of input strings and their transliteration via labels.\n",
        "    :param train_source_strings: a list of strings, one str per example\n",
        "    :param train_target_strings: a list of strings, one str per example\n",
        "    :return: learnt parameters, or any object you like (it will be passed to the classify function)\n",
        "    \"\"\"\n",
        "\n",
        "    ngram_lvl = 3\n",
        "    def obtain_train_dicts(train_source_strings, train_target_strings,\n",
        "                            ngram_lvl):\n",
        "        ngrams_dict = col.defaultdict(lambda: col.defaultdict(int))\n",
        "        for src_str,dst_str in zip(train_source_strings,\n",
        "                                        train_target_strings):\n",
        "            try:\n",
        "                src_ngrams = [src_str[i:i+ngram_lvl] for i in\n",
        "                                range(len(src_str)-ngram_lvl+1)]\n",
        "                dst_ngrams = [dst_str[i:i+ngram_lvl] for i in\n",
        "                                range(len(dst_str)-ngram_lvl+1)]\n",
        "            except TypeError as e:\n",
        "                print(src_ngrams, dst_ngrams)\n",
        "                print(e)\n",
        "                raise StopIteration\n",
        "            for src_ngram in src_ngrams:\n",
        "                for dst_ngram in dst_ngrams:\n",
        "                    ngrams_dict[src_ngram][dst_ngram] += 1\n",
        "        return ngrams_dict\n",
        "        \n",
        "    ngrams_dict = col.defaultdict(lambda: col.defaultdict(int))\n",
        "    for nl in range(1, ngram_lvl+1):\n",
        "        ngrams_dict.update(\n",
        "            obtain_train_dicts(train_source_strings,\n",
        "                            train_target_strings, nl))\n",
        "    return ngrams_dict \n",
        "\n",
        "\n",
        "def baseline_classify(strings: List[str], params: Any) -> List[str]:\n",
        "    \"\"\"\n",
        "    Classify strings given previously learnt parameters.\n",
        "    :param strings: strings to classify\n",
        "    :param params: parameters received from train function\n",
        "    :return: list of lists of predicted transliterated strings\n",
        "      (for each source string -> [top_1 prediction, .., top_k prediction]\n",
        "        if it is possible to generate more than one, otherwise\n",
        "        -> [prediction])\n",
        "        corresponding to the given list of strings\n",
        "    \"\"\"\n",
        "       \n",
        "    def predict_one_sample(sample, train_dict, ngram_lvl=1):\n",
        "        ngrams = [sample[i:i+ngram_lvl] for i in\n",
        " range(0,(len(sample) // ngram_lvl * ngram_lvl)-ngram_lvl+1, ngram_lvl)] +\\\n",
        "                 ([] if len(sample) % ngram_lvl == 0 else\n",
        "                    [sample[-(len(sample) % ngram_lvl):]])\n",
        "        prediction = ''\n",
        "        for ngram in ngrams:\n",
        "            ngram_dict = train_dict[ngram]\n",
        "            if len(ngram_dict.keys()) == 0:\n",
        "                prediction += '?'*len(ngram)\n",
        "            else:\n",
        "                prediction += max(ngram_dict, key=lambda k: ngram_dict[k])\n",
        "        return prediction \n",
        "    \n",
        "    ngram_lvl = 3\n",
        "    predictions = []\n",
        "    ngrams_dict = params\n",
        "    for string in strings:\n",
        "        top_1_pred = predict_one_sample(string, ngrams_dict,\n",
        "                                                ngram_lvl)\n",
        "        predictions.append([top_1_pred])\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "U0B2Vyk-4GvE",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:13:55.498704Z",
          "iopub.execute_input": "2023-04-11T19:13:55.499820Z",
          "iopub.status.idle": "2023-04-11T19:13:55.526273Z",
          "shell.execute_reply.started": "2023-04-11T19:13:55.499731Z",
          "shell.execute_reply": "2023-04-11T19:13:55.525459Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation code"
      ],
      "metadata": {
        "id": "r7p9Nac-4X8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PREDS_FNAME = \"preds_translit_baseline.tsv\"\n",
        "SCORED_PARTS = ('train', 'dev', 'train_small', 'dev_small', 'test')\n",
        "TRANSLIT_PATH = \"TRANSLIT\""
      ],
      "metadata": {
        "id": "aOz9Miec58Tb",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:13:55.531234Z",
          "iopub.execute_input": "2023-04-11T19:13:55.532075Z",
          "iopub.status.idle": "2023-04-11T19:13:55.543550Z",
          "shell.execute_reply.started": "2023-04-11T19:13:55.532036Z",
          "shell.execute_reply": "2023-04-11T19:13:55.542650Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import codecs\n",
        "from pandas import read_csv\n",
        "\n",
        "def load_dataset(data_dir_path=None, parts: List[str] = SCORED_PARTS):\n",
        "    part2ixy = {}\n",
        "    for part in parts:\n",
        "        path = os.path.join(data_dir_path, f'{part}.tsv')\n",
        "        with open(path, 'r', encoding='utf-8') as rf:\n",
        "            # first line is a header of the corresponding columns\n",
        "            lines = rf.readlines()[1:]\n",
        "            col_count = len(lines[0].strip('\\n').split('\\t'))\n",
        "            if col_count == 2:\n",
        "                strings, transliterations = zip(\n",
        "                    *list(map(lambda l: l.strip('\\n').split('\\t'), lines))\n",
        "                )\n",
        "            elif col_count == 1:\n",
        "                strings = list(map(lambda l: l.strip('\\n'), lines))\n",
        "                transliterations = None\n",
        "            else:\n",
        "                raise ValueError(\"wrong amount of columns\")\n",
        "        part2ixy[part] = (\n",
        "            [f'{part}/{i}' for i in range(len(strings))],\n",
        "            strings, transliterations,\n",
        "        )\n",
        "    return part2ixy\n",
        "\n",
        "\n",
        "def load_transliterations_only(data_dir_path=None, parts: List[str] = SCORED_PARTS):\n",
        "    part2iy = {}\n",
        "    for part in parts:\n",
        "        path = os.path.join(data_dir_path, f'{part}.tsv')\n",
        "        with open(path, 'r', encoding='utf-8') as rf:\n",
        "            # first line is a header of the corresponding columns\n",
        "            lines = rf.readlines()[1:]\n",
        "            col_count = len(lines[0].strip('\\n').split('\\t'))\n",
        "            n_lines = len(lines)\n",
        "            if col_count == 2:\n",
        "                transliterations = [l.strip('\\n').split('\\t')[1] for l in lines]\n",
        "            elif col_count == 1:\n",
        "                transliterations = None\n",
        "            else:\n",
        "                raise ValueError(\"Wrong amount of columns\")\n",
        "        part2iy[part] = (\n",
        "            [f'{part}/{i}' for i in range(n_lines)],\n",
        "            transliterations,\n",
        "        )\n",
        "    return part2iy\n",
        "\n",
        "\n",
        "def save_preds(preds, preds_fname):\n",
        "    \"\"\"\n",
        "    Save classifier predictions in format appropriate for scoring.\n",
        "    \"\"\"\n",
        "    with codecs.open(preds_fname, 'w') as outp:\n",
        "        for idx, preds in preds:\n",
        "            print(idx, *preds, sep='\\t', file=outp)\n",
        "    print('Predictions saved to %s' % preds_fname)\n",
        "\n",
        "\n",
        "def load_preds(preds_fname, top_k=1):\n",
        "    \"\"\"\n",
        "    Load classifier predictions in format appropriate for scoring.\n",
        "    \"\"\"\n",
        "    kwargs = {\n",
        "        \"filepath_or_buffer\": preds_fname,\n",
        "        \"names\": [\"id\", \"pred\"],\n",
        "        \"sep\": '\\t',\n",
        "    }\n",
        "\n",
        "    pred_ids = list(read_csv(**kwargs, usecols=[\"id\"])[\"id\"])\n",
        "\n",
        "    pred_y = {\n",
        "        pred_id: [y]\n",
        "        for pred_id, y in zip(\n",
        "            pred_ids, read_csv(**kwargs, usecols=[\"pred\"])[\"pred\"]\n",
        "        )\n",
        "    }\n",
        "\n",
        "    for y in pred_y.values():\n",
        "        assert len(y) == top_k\n",
        "\n",
        "    return pred_ids, pred_y\n",
        "\n",
        "\n",
        "def compute_hit_k(preds, k=10):\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "def compute_mrr(preds):\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "def compute_acc_1(preds, true):\n",
        "    right_answers = 0\n",
        "    bonus = 0\n",
        "    for pred, y in zip(preds, true):\n",
        "        if pred[0] == y:\n",
        "            right_answers += 1\n",
        "        elif pred[0] != pred[0] and y == 'нань':\n",
        "            print('Your test file contained empty string, skipping %f and %s' % (pred[0], y))\n",
        "            bonus += 1 # bugfix: skip empty line in test\n",
        "    return right_answers / (len(preds) - bonus)\n",
        "\n",
        "\n",
        "def score(preds, true):\n",
        "    assert len(preds) == len(true), 'inconsistent amount of predictions and ground truth answers'\n",
        "    acc_1 = compute_acc_1(preds, true)\n",
        "    return {'acc@1': acc_1}\n",
        "\n",
        "\n",
        "def score_preds(preds_path, data_dir, parts=SCORED_PARTS):\n",
        "    part2iy = load_transliterations_only(data_dir, parts=parts)\n",
        "    pred_ids, pred_dict = load_preds(preds_path)\n",
        "    # pred_dict = {i:y for i,y in zip(pred_ids, pred_y)}\n",
        "    scores = {}\n",
        "    for part, (true_ids, true_y) in part2iy.items():\n",
        "        if true_y is None:\n",
        "            print('no labels for %s set' % part)\n",
        "            continue\n",
        "        pred_y = [pred_dict[i] for i in true_ids]\n",
        "        score_values = score(pred_y, true_y)\n",
        "        acc_1 = score_values['acc@1']\n",
        "        print('%s set accuracy@1: %.2f' % (part, acc_1))\n",
        "        scores[part] = score_values \n",
        "    return scores"
      ],
      "metadata": {
        "id": "SdbtMBxd52yX",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:13:55.546963Z",
          "iopub.execute_input": "2023-04-11T19:13:55.547251Z",
          "iopub.status.idle": "2023-04-11T19:13:55.569615Z",
          "shell.execute_reply.started": "2023-04-11T19:13:55.547225Z",
          "shell.execute_reply": "2023-04-11T19:13:55.568861Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and predict results"
      ],
      "metadata": {
        "id": "Juwpml7Z8tbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "def train_and_predict(translit_path, scored_parts):\n",
        "    top_k = 1\n",
        "    part2ixy = load_dataset(translit_path, parts=scored_parts)\n",
        "    train_ids, train_strings, train_transliterations = part2ixy['train']\n",
        "    print('\\nTraining classifier on %d examples from train set ...' % len(train_strings))\n",
        "    st = time()\n",
        "    params = baseline_train(train_strings, train_transliterations)\n",
        "    print('Classifier trained in %.2fs' % (time() - st))\n",
        "\n",
        "    allpreds = []\n",
        "    for part, (ids, x, y) in part2ixy.items():\n",
        "        print('\\nClassifying %s set with %d examples ...' % (part, len(x)))\n",
        "        st = time()\n",
        "        preds = baseline_classify(x, params)\n",
        "        print('%s set classified in %.2fs' % (part, time() - st))\n",
        "        count_of_values = list(map(len, preds))\n",
        "        assert np.all(np.array(count_of_values) == top_k)\n",
        "        #score(preds, y)\n",
        "        allpreds.extend(zip(ids, preds))\n",
        "\n",
        "    save_preds(allpreds, preds_fname=PREDS_FNAME)\n",
        "    print('\\nChecking saved predictions ...')\n",
        "    return score_preds(preds_path=PREDS_FNAME, data_dir=translit_path, parts=scored_parts)"
      ],
      "metadata": {
        "id": "VvOmwqtQ4URL",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:13:55.571035Z",
          "iopub.execute_input": "2023-04-11T19:13:55.571704Z",
          "iopub.status.idle": "2023-04-11T19:13:55.586351Z",
          "shell.execute_reply.started": "2023-04-11T19:13:55.571667Z",
          "shell.execute_reply": "2023-04-11T19:13:55.585264Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_predict(TRANSLIT_PATH, SCORED_PARTS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZ1GS3l28MRY",
        "outputId": "7b494ead-5511-4a89-a8af-b616d66817c2",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:13:55.587949Z",
          "iopub.execute_input": "2023-04-11T19:13:55.588840Z",
          "iopub.status.idle": "2023-04-11T19:14:45.466996Z",
          "shell.execute_reply.started": "2023-04-11T19:13:55.588803Z",
          "shell.execute_reply": "2023-04-11T19:14:45.465913Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nClassifier trained in 3.68s\n\nClassifying train set with 105371 examples ...\ntrain set classified in 27.17s\n\nClassifying dev set with 26342 examples ...\ndev set classified in 7.33s\n\nClassifying train_small set with 2000 examples ...\ntrain_small set classified in 0.51s\n\nClassifying dev_small set with 2000 examples ...\ndev_small set classified in 0.50s\n\nClassifying test set with 32926 examples ...\ntest set classified in 8.39s\nPredictions saved to preds_translit_baseline.tsv\n\nChecking saved predictions ...\ntrain set accuracy@1: 0.33\ndev set accuracy@1: 0.31\ntrain_small set accuracy@1: 0.34\ndev_small set accuracy@1: 0.32\nno labels for test set\n",
          "output_type": "stream"
        },
        {
          "execution_count": 8,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'train': {'acc@1': 0.32907536229133255},\n 'dev': {'acc@1': 0.3112899552046162},\n 'train_small': {'acc@1': 0.3365},\n 'dev_small': {'acc@1': 0.323}}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer-based approach"
      ],
      "metadata": {
        "id": "VwXaPC4LiUMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To implement your algorithm, use the template code, which needs to be modified.\n",
        "\n",
        "First, you need to add some details in the code of the Transformer architecture, implement the methods of the class `LrScheduler`, which is responsible for updating the learning rate during training.\n",
        "Next, you need to select the hyperparameters for the model according to the proposed guide."
      ],
      "metadata": {
        "id": "iM-9cKhbidfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Levenshtein"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nePd6qR5_sC-",
        "outputId": "f6cfe0fc-e9a5-4773-ad9d-c400308c1922",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:14:45.468686Z",
          "iopub.execute_input": "2023-04-11T19:14:45.469418Z",
          "iopub.status.idle": "2023-04-11T19:14:56.040616Z",
          "shell.execute_reply.started": "2023-04-11T19:14:45.469369Z",
          "shell.execute_reply": "2023-04-11T19:14:56.039398Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: Levenshtein in /opt/conda/lib/python3.7/site-packages (0.20.9)\nRequirement already satisfied: rapidfuzz<3.0.0,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from Levenshtein) (2.15.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools as it\n",
        "import collections as col\n",
        "import random\n",
        "import os\n",
        "import copy\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import datetime, time\n",
        "\n",
        "import copy\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as torch_data\n",
        "import itertools as it\n",
        "import collections as col\n",
        "import random\n",
        "\n",
        "import Levenshtein as le"
      ],
      "metadata": {
        "id": "LQ5sfyjhBawp",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:14:56.045719Z",
          "iopub.execute_input": "2023-04-11T19:14:56.046041Z",
          "iopub.status.idle": "2023-04-11T19:14:58.440588Z",
          "shell.execute_reply.started": "2023-04-11T19:14:56.046009Z",
          "shell.execute_reply": "2023-04-11T19:14:58.439591Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load dataset and embeddings"
      ],
      "metadata": {
        "id": "r6enjcHrD_0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_datasets(data_dir_path, parts):\n",
        "    datasets = {}\n",
        "    for part in parts:\n",
        "        path = os.path.join(data_dir_path, f'{part}.tsv')\n",
        "        datasets[part] = pd.read_csv(path, sep='\\t', na_filter=False)\n",
        "        print(f'Loaded {part} dataset, length: {len(datasets[part])}')\n",
        "    return datasets"
      ],
      "metadata": {
        "id": "RmWG0dDUCFto",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:14:58.442042Z",
          "iopub.execute_input": "2023-04-11T19:14:58.442656Z",
          "iopub.status.idle": "2023-04-11T19:14:58.448590Z",
          "shell.execute_reply.started": "2023-04-11T19:14:58.442619Z",
          "shell.execute_reply": "2023-04-11T19:14:58.447242Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder:\n",
        "    def __init__(self, load_dir_path=None):\n",
        "        self.lang_keys = ['en', 'ru']\n",
        "        self.directions = ['id2token', 'token2id']\n",
        "        self.service_token_names = {\n",
        "            'pad_token': '<pad>',\n",
        "            'start_token': '<start>',\n",
        "            'unk_token': '<unk>',\n",
        "            'end_token': '<end>'\n",
        "        }\n",
        "        service_id2token = dict(enumerate(self.service_token_names.values()))\n",
        "        service_token2id ={v:k for k,v in service_id2token.items()}\n",
        "        self.service_vocabs = dict(zip(self.directions,\n",
        "                                       [service_id2token, service_token2id]))\n",
        "        if load_dir_path is None:\n",
        "            self.vocabs = {}\n",
        "            for lk in self.lang_keys:\n",
        "                self.vocabs[lk] = copy.deepcopy(self.service_vocabs)\n",
        "        else:\n",
        "            self.vocabs = self.load_vocabs(load_dir_path)\n",
        "    def load_vocabs(self, load_dir_path):\n",
        "        vocabs = {}\n",
        "        load_path = os.path.join(load_dir_path, 'vocabs')\n",
        "        for lk in self.lang_keys:\n",
        "            vocabs[lk] = {}\n",
        "            for d in self.directions:\n",
        "                columns = d.split('2')\n",
        "                print(lk, d)\n",
        "                df = pd.read_csv(os.path.join(load_path, f'{lk}_{d}'))\n",
        "                vocabs[lk][d] = dict(zip(*[df[c] for c in columns]))\n",
        "        return vocabs\n",
        "    \n",
        "    def save_vocabs(self, save_dir_path):\n",
        "        save_path = os.path.join(save_dir_path, 'vocabs')\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        for lk in self.lang_keys:\n",
        "            for d in self.directions:\n",
        "                columns = d.split('2')\n",
        "                pd.DataFrame(data=self.vocabs[lk][d].items(),\n",
        "                    columns=columns).to_csv(os.path.join(save_path, f'{lk}_{d}'),\n",
        "                                                index=False,\n",
        "                                                sep=',')\n",
        "    def make_vocabs(self, data_df):\n",
        "        for lk in self.lang_keys:\n",
        "            tokens = col.Counter(''.join(list(it.chain(*data_df[lk])))).keys()\n",
        "            part_id2t = dict(enumerate(tokens, start=len(self.service_token_names)))\n",
        "            part_t2id = {k:v for v,k in part_id2t.items()}\n",
        "            part_vocabs = [part_id2t, part_t2id]\n",
        "            for i in range(len(self.directions)):\n",
        "                self.vocabs[lk][self.directions[i]].update(part_vocabs[i])\n",
        "                \n",
        "        self.src_vocab_size = len(self.vocabs['en']['id2token'])\n",
        "        self.tgt_vocab_size = len(self.vocabs['ru']['id2token'])\n",
        "                \n",
        "    def frame(self, sample, start_token=None, end_token=None):\n",
        "        if start_token is None:\n",
        "            start_token=self.service_token_names['start_token']\n",
        "        if end_token is None:\n",
        "            end_token=self.service_token_names['end_token']\n",
        "        return [start_token] + sample + [end_token]\n",
        "    def token2id(self, samples, frame, lang_key):\n",
        "        if frame:\n",
        "            samples = list(map(self.frame, samples))\n",
        "        vocab = self.vocabs[lang_key]['token2id']\n",
        "        return list(map(lambda s:\n",
        "                        [vocab[t] if t in vocab.keys() else vocab[self.service_token_names['unk_token']]\n",
        "                         for t in s], samples))\n",
        "    \n",
        "    def unframe(self, sample, start_token=None, end_token=None):\n",
        "        if start_token is None:\n",
        "            start_token=self.service_vocabs['token2id'][self.service_token_names['start_token']]\n",
        "        if end_token is None:\n",
        "            end_token=self.service_vocabs['token2id'][self.service_token_names['end_token']]\n",
        "        pad_token=self.service_vocabs['token2id'][self.service_token_names['pad_token']]\n",
        "        return list(it.takewhile(lambda e: e != end_token and e != pad_token, sample[1:]))\n",
        "    def id2token(self, samples, unframe, lang_key):\n",
        "        if unframe:\n",
        "            samples = list(map(self.unframe, samples))\n",
        "        vocab = self.vocabs[lang_key]['id2token']\n",
        "        return list(map(lambda s:\n",
        "                        [vocab[idx] if idx in vocab.keys() else self.service_token_names['unk_token'] for idx in s], samples))\n",
        "\n",
        "\n",
        "class TranslitData(torch_data.Dataset):\n",
        "    def __init__(self, source_strings, target_strings,\n",
        "                text_encoder):\n",
        "        super(TranslitData, self).__init__()\n",
        "        self.source_strings = source_strings\n",
        "        self.text_encoder = text_encoder\n",
        "        if target_strings is not None:\n",
        "            assert len(source_strings) == len(target_strings)\n",
        "            self.target_strings = target_strings\n",
        "        else:\n",
        "            self.target_strings = None\n",
        "    def __len__(self):\n",
        "        return len(self.source_strings)\n",
        "    def __getitem__(self, idx):\n",
        "        src_str = self.source_strings[idx]\n",
        "        encoder_input = self.text_encoder.token2id([list(src_str)], frame=True, lang_key='en')[0]\n",
        "        if self.target_strings is not None:\n",
        "            tgt_str = self.target_strings[idx]\n",
        "            tmp = self.text_encoder.token2id([list(tgt_str)], frame=True, lang_key='ru')[0]\n",
        "            decoder_input = tmp[:-1]\n",
        "            decoder_target = tmp[1:]\n",
        "            return (encoder_input, decoder_input, decoder_target)\n",
        "        else:\n",
        "            return (encoder_input,)\n",
        "\n",
        "\n",
        "class BatchSampler(torch_data.BatchSampler):\n",
        "    def __init__(self, sampler, batch_size, drop_last, shuffle_each_epoch):\n",
        "        super(BatchSampler, self).__init__(sampler, batch_size, drop_last)\n",
        "        self.batches = []\n",
        "        for b in super(BatchSampler, self).__iter__():\n",
        "            self.batches.append(b)\n",
        "        self.shuffle_each_epoch = shuffle_each_epoch\n",
        "        if self.shuffle_each_epoch:\n",
        "            random.shuffle(self.batches)\n",
        "        self.index = 0\n",
        "        #print(f'Batches collected: {len(self.batches)}')\n",
        "    def __iter__(self):\n",
        "        self.index = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self.index == len(self.batches):\n",
        "            if self.shuffle_each_epoch:\n",
        "                random.shuffle(self.batches)\n",
        "            raise StopIteration\n",
        "        else:\n",
        "            batch = self.batches[self.index]\n",
        "            self.index += 1\n",
        "            return batch\n",
        "\n",
        "def collate_fn(batch_list):\n",
        "    '''batch_list can store either 3 components:\n",
        "        encoder_inputs, decoder_inputs, decoder_targets\n",
        "        or single component: encoder_inputs'''\n",
        "    components = list(zip(*batch_list))\n",
        "    batch_tensors = []\n",
        "    for data in components:\n",
        "        max_len = max([len(sample) for sample in data])\n",
        "        #print(f'Maximum length in batch = {max_len}')\n",
        "        sample_tensors = [torch.tensor(s, requires_grad=False, dtype=torch.int64)\n",
        "                         for s in data]\n",
        "        batch_tensors.append(nn.utils.rnn.pad_sequence(\n",
        "            sample_tensors,\n",
        "            batch_first=True, padding_value=0))\n",
        "    return tuple(batch_tensors) \n",
        "\n",
        "\n",
        "def create_dataloader(source_strings, target_strings,\n",
        "                      text_encoder, batch_size,\n",
        "                      shuffle_batches_each_epoch):\n",
        "    '''target_strings parameter can be None'''\n",
        "    dataset = TranslitData(source_strings, target_strings,\n",
        "                                text_encoder=text_encoder)\n",
        "    seq_sampler = torch_data.SequentialSampler(dataset)\n",
        "    batch_sampler = BatchSampler(seq_sampler, batch_size=batch_size,\n",
        "                                drop_last=False,\n",
        "                                shuffle_each_epoch=shuffle_batches_each_epoch)\n",
        "    dataloader = torch_data.DataLoader(dataset,\n",
        "                                       batch_sampler=batch_sampler,\n",
        "                                       collate_fn=collate_fn)\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "-gMDFNVt-nMw",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:14:58.451197Z",
          "iopub.execute_input": "2023-04-11T19:14:58.452004Z",
          "iopub.status.idle": "2023-04-11T19:14:58.485977Z",
          "shell.execute_reply.started": "2023-04-11T19:14:58.451959Z",
          "shell.execute_reply": "2023-04-11T19:14:58.485010Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metric function"
      ],
      "metadata": {
        "id": "-Qsehb_eERRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(predicted_strings, target_strings, metrics):\n",
        "    metric_values = {}\n",
        "    for m in metrics:\n",
        "        if m == 'acc@1':\n",
        "            metric_values[m] = sum(predicted_strings == target_strings) / len(target_strings)\n",
        "        elif m =='mean_ld@1':\n",
        "            metric_values[m] =\\\n",
        "                np.mean(list(map(lambda e: le.distance(*e), zip(predicted_strings, target_strings))))\n",
        "        else: \n",
        "            raise ValueError(f'Unknown metric: {m}')\n",
        "    return metric_values"
      ],
      "metadata": {
        "id": "MN4I60GqETB0",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:14:58.487547Z",
          "iopub.execute_input": "2023-04-11T19:14:58.487928Z",
          "iopub.status.idle": "2023-04-11T19:14:58.499087Z",
          "shell.execute_reply.started": "2023-04-11T19:14:58.487882Z",
          "shell.execute_reply": "2023-04-11T19:14:58.498142Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Positional Encoding"
      ],
      "metadata": {
        "id": "UiYl5XsdkmZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you remember, Transformer treats an input sequence of elements as a time series. Since the Encoder inside the Transformer simultaneously processes the entire input sequence, the information about the position of the element needs to be encoded inside its embedding, since it is not identified in any other way inside the model. That is why the PositionalEncoding layer is used, which sums embeddings with a vector of the same dimension.\n",
        "Let the matrix of these vectors for each position of the time series be denoted as $PE$. Then the elements of the matrix are:\n",
        "\n",
        "$$ PE_{(pos,2i)} = \\sin{(pos/10000^{2i/d_{model}})}$$\n",
        "$$ PE_{(pos,2i+1)} = \\cos{(pos/10000^{2i/d_{model}})}$$\n",
        "\n",
        "where $pos$ - is the position, $i$ - index of the component of the corresponging vector, $d_{model}$ - dimension of each vector. Thus, even components represent sine values, and odd ones represent cosine values with different arguments.\n",
        "\n",
        "In this task you are required to implement these formulas inside the class constructor *PositionalEncoding* in the main file ``translit.py``, which you are to upload. To run the test use the following function:\n",
        "\n",
        "`test_positional_encoding()`\n",
        "\n",
        "Make sure that there is no any `AssertionError`!\n"
      ],
      "metadata": {
        "id": "vkNaSzwrkpf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "    def __init__(self, hidden_size, vocab_size):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.emb_layer = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.emb_layer(x)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, hidden_size, max_len=512):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, hidden_size, requires_grad=False)\n",
        "        pos = torch.arange(0, max_len).unsqueeze(1).expand(max_len, hidden_size)\n",
        "        ten_pow = torch.exp(math.log(10 ** 4) *  torch.arange(0, hidden_size, 2) / hidden_size)\n",
        "        pe[:,0::2] = torch.sin(pos[:,0::2] / ten_pow)\n",
        "        pe[:,1::2] = torch.cos(pos[:,1::2] / ten_pow)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: shape (batch size, sequence length, hidden size)\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x"
      ],
      "metadata": {
        "id": "Rm4g1vybAKZs",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:14:58.500477Z",
          "iopub.execute_input": "2023-04-11T19:14:58.500836Z",
          "iopub.status.idle": "2023-04-11T19:14:58.511085Z",
          "shell.execute_reply.started": "2023-04-11T19:14:58.500801Z",
          "shell.execute_reply": "2023-04-11T19:14:58.509941Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_positional_encoding():\n",
        "    pe = PositionalEncoding(max_len=3, hidden_size=4)\n",
        "    res_1 = torch.tensor([[[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
        "                           [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
        "                           [ 0.9093, -0.4161,  0.0200,  0.9998]]])\n",
        "    # print(pe.pe - res_1)\n",
        "    assert torch.all(torch.abs(pe.pe - res_1) < 1e-4).item()\n",
        "    print('Test is passed!')"
      ],
      "metadata": {
        "id": "hBk6uxQyCqt-",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:14:58.512511Z",
          "iopub.execute_input": "2023-04-11T19:14:58.512879Z",
          "iopub.status.idle": "2023-04-11T19:14:58.525864Z",
          "shell.execute_reply.started": "2023-04-11T19:14:58.512840Z",
          "shell.execute_reply": "2023-04-11T19:14:58.524953Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_positional_encoding()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwN9Qk_NCw-K",
        "outputId": "da0e992c-da90-4000-d179-c4e82ec49825",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:14:58.527450Z",
          "iopub.execute_input": "2023-04-11T19:14:58.527901Z",
          "iopub.status.idle": "2023-04-11T19:14:58.644229Z",
          "shell.execute_reply.started": "2023-04-11T19:14:58.527863Z",
          "shell.execute_reply": "2023-04-11T19:14:58.643019Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Test is passed!\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LayerNorm"
      ],
      "metadata": {
        "id": "g026bdkrEtiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"Layer Normalization layer\"\n",
        "\n",
        "    def __init__(self, hidden_size, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.gain = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.gain * (x - mean) / (std + self.eps) + self.bias"
      ],
      "metadata": {
        "id": "dIMy52O9Es0K",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:14:58.645697Z",
          "iopub.execute_input": "2023-04-11T19:14:58.646327Z",
          "iopub.status.idle": "2023-04-11T19:14:58.653705Z",
          "shell.execute_reply.started": "2023-04-11T19:14:58.646288Z",
          "shell.execute_reply": "2023-04-11T19:14:58.652625Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SublayerConnection"
      ],
      "metadata": {
        "id": "TEpsKqLwE3hB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer normalization.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.layer_norm = LayerNorm(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        return self.layer_norm(x + self.dropout(sublayer(x)))\n",
        "\n",
        "def padding_mask(x, pad_idx=0):\n",
        "    assert len(x.size()) >= 2\n",
        "    return (x != pad_idx).unsqueeze(-2)\n",
        "\n",
        "def look_ahead_mask(size):\n",
        "    \"Mask out the right context\"\n",
        "    attn_shape = (1, size, size)\n",
        "    look_ahead_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(look_ahead_mask) == 0\n",
        "\n",
        "def compositional_mask(x, pad_idx=0):\n",
        "    pm = padding_mask(x, pad_idx=pad_idx)\n",
        "    seq_length = x.size(-1)\n",
        "    result_mask = pm & \\\n",
        "                  look_ahead_mask(seq_length).type_as(pm.data)\n",
        "    return result_mask"
      ],
      "metadata": {
        "id": "2yuMxRinE3uH",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:14:58.655451Z",
          "iopub.execute_input": "2023-04-11T19:14:58.655881Z",
          "iopub.status.idle": "2023-04-11T19:14:58.666173Z",
          "shell.execute_reply.started": "2023-04-11T19:14:58.655783Z",
          "shell.execute_reply": "2023-04-11T19:14:58.665149Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FeedForward"
      ],
      "metadata": {
        "id": "Kh86csH_FCyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, hidden_size, ff_hidden_size, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.pre_linear = nn.Linear(hidden_size, ff_hidden_size)\n",
        "        self.post_linear = nn.Linear(ff_hidden_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.post_linear(self.dropout(F.relu(self.pre_linear(x))))\n",
        "\n",
        "def clone_layer(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "metadata": {
        "id": "4zy1PNwlGIEX",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:14:58.667862Z",
          "iopub.execute_input": "2023-04-11T19:14:58.668362Z",
          "iopub.status.idle": "2023-04-11T19:14:58.678200Z",
          "shell.execute_reply.started": "2023-04-11T19:14:58.668327Z",
          "shell.execute_reply": "2023-04-11T19:14:58.677247Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  MultiHeadAttention"
      ],
      "metadata": {
        "id": "IwoQ_X8ylJYN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Then you are required to implement `attention` method in the class  `MultiHeadAttention`. The MultiHeadAttention layer takes as input  query vectors, key and value vectors for each step of the sequence of matrices  Q,K,V correspondingly. Each key vector, value vector, and query vector is obtained as a result of linear projection using one of three trained vector parameter matrices from the previous layer. This semantics can be represented in the form of formulas:\n",
        "$$\n",
        "Attention(Q, K, V)=softmax\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V\\\\\n",
        "$$\n",
        "\n",
        "$$\n",
        "MultiHead(Q, K, V) = Concat\\left(head_1, ... , head_h\\right) W^O\\\\\n",
        "$$\n",
        "\n",
        "$$\n",
        "head_i=Attention\\left(Q W_i^Q, K W_i^K, V W_i^V\\right)\\\\\n",
        "$$\n",
        "$h$ - the number of attention heads - parallel sub-layers for Scaled Dot-Product Attention on a vector of smaller dimension ($d_{k} = d_{q} = d_{v} = d_{model} / h$). \n",
        "The logic of  \\texttt{MultiHeadAttention} is presented in the picture (from original  [paper](https://arxiv.org/abs/1706.03762)):\n",
        "\n",
        "![](https://lilianweng.github.io/lil-log/assets/images/transformer.png)\n",
        "\n",
        "\n",
        "Inside a method `attention` you are required to create a dropout layer from  MultiHeadAttention class constructor. Dropout layer is to be applied directly on the attention weights - the result of softmax operation. Value of drop probability  can be regulated in the train in the `model_config['dropout']['attention']`.\n",
        "\n",
        "The correctness of implementation can be checked with\n",
        "`test_multi_head_attention()`\n",
        "\n"
      ],
      "metadata": {
        "id": "SYGVEp3mkgNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, hidden_size, dropout=None):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert hidden_size % n_heads == 0\n",
        "        self.head_hidden_size = hidden_size // n_heads\n",
        "        self.n_heads = n_heads\n",
        "        self.linears = clone_layer(nn.Linear(hidden_size, hidden_size), 4)\n",
        "        self.attn_weights = None\n",
        "        self.dropout = dropout\n",
        "        if self.dropout is not None:\n",
        "            self.dropout_layer = nn.Dropout(p=self.dropout)\n",
        "\n",
        "    def attention(self, query, key, value, mask):\n",
        "        \"\"\"Compute 'Scaled Dot Product Attention'\n",
        "            query, key and value tensors have the same shape:\n",
        "                (batch size, number of heads, sequence length, head hidden size)\n",
        "            mask shape: (batch size, 1, sequence length, sequence length)\n",
        "                '1' dimension value will be broadcasted to number of heads inside your operations\n",
        "            mask should be applied before using softmax to get attn_weights\n",
        "        \"\"\"\n",
        "        ## attn_weights shape: (batch size, number of heads, sequence length, sequence length)\n",
        "        ## output shape: (batch size, number of heads, sequence length, head hidden size)\n",
        "        ## TODO: provide your implementation here\n",
        "        d = self.head_hidden_size\n",
        "        scores = query @ key.transpose(-2, -1) / math.sqrt(d)\n",
        "        if mask is not None:\n",
        "              scores.masked_fill_(mask == 0, -1e9)\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        if self.dropout is not None:\n",
        "              attn_weights = self.dropout_layer(attn_weights)\n",
        "        output = attn_weights @ value\n",
        "        ## don't forget to apply dropout to attn_weights if self.dropout is not None\n",
        "        return output, attn_weights\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # Split vectors for different attention heads (from hidden_size => n_heads x head_hidden_size)\n",
        "        # and do separate linear projection, for separate trainable weights\n",
        "        query, key, value = \\\n",
        "            [l(x).view(batch_size, -1, self.n_heads, self.head_hidden_size).transpose(1, 2)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "\n",
        "        x, self.attn_weights = self.attention(query, key, value, mask=mask)\n",
        "        # x shape: (batch size, number of heads, sequence length, head hidden size)\n",
        "        # self.attn_weights shape: (batch size, number of heads, sequence length, sequence length)\n",
        "\n",
        "        # Concatenate the output of each head\n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "            .view(batch_size, -1, self.n_heads * self.head_hidden_size)\n",
        "\n",
        "        return self.linears[-1](x)"
      ],
      "metadata": {
        "id": "5q7mpdjnAVHP",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:14:58.679793Z",
          "iopub.execute_input": "2023-04-11T19:14:58.680526Z",
          "iopub.status.idle": "2023-04-11T19:14:58.693306Z",
          "shell.execute_reply.started": "2023-04-11T19:14:58.680490Z",
          "shell.execute_reply": "2023-04-11T19:14:58.692355Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_multi_head_attention():\n",
        "    mha = MultiHeadAttention(n_heads=1, hidden_size=5, dropout=None)\n",
        "    # batch_size == 2, sequence length == 3, hidden_size == 5\n",
        "    # query = torch.arange(150).reshape(2, 3, 5)\n",
        "    query = torch.tensor([[[[ 0.64144618, -0.95817388,  0.37432297,  0.58427106,\n",
        "          -0.94668716]],\n",
        "        [[-0.23199289,  0.66329209, -0.46507035, -0.54272512,\n",
        "          -0.98640698]],\n",
        "        [[ 0.07546638, -0.09277002,  0.20107185, -0.97407381,\n",
        "          -0.27713414]]],\n",
        "       [[[ 0.14727783,  0.4747886 ,  0.44992016, -0.2841419 ,\n",
        "          -0.81820319]],\n",
        "        [[-0.72324994,  0.80643179, -0.47655449,  0.45627872,\n",
        "           0.60942404]],\n",
        "        [[ 0.61712569, -0.62947282, -0.95215713, -0.38721959,\n",
        "          -0.73289725]]]])\n",
        "    key = torch.tensor([[[[-0.81759856, -0.60049991, -0.05923424,  0.51898901,\n",
        "          -0.3366209 ]],\n",
        "        [[ 0.83957818, -0.96361722,  0.62285191,  0.93452467,\n",
        "           0.51219613]],\n",
        "        [[-0.72758847,  0.41256154,  0.00490795,  0.59892503,\n",
        "          -0.07202049]]],\n",
        "       [[[ 0.72315339, -0.49896314,  0.94254637, -0.54356006,\n",
        "          -0.04837949]],\n",
        "        [[ 0.51759322, -0.43927061, -0.59924184,  0.92241702,\n",
        "          -0.86811696]],\n",
        "        [[-0.54322046, -0.92323003, -0.827746  ,  0.90842783,\n",
        "           0.88428119]]]])\n",
        "    value = torch.tensor([[[[-0.83895431,  0.805027  ,  0.22298283, -0.84849915,\n",
        "          -0.34906026]],\n",
        "        [[-0.02899652, -0.17456128, -0.17535998, -0.73160314,\n",
        "          -0.13468061]],\n",
        "        [[ 0.75234265,  0.02675947,  0.84766286, -0.5475651 ,\n",
        "          -0.83319316]]],\n",
        "       [[[-0.47834413,  0.34464645, -0.41921457,  0.33867964,\n",
        "           0.43470836]],\n",
        "        [[-0.99000979,  0.10220893, -0.4932273 ,  0.95938905,\n",
        "           0.01927012]],\n",
        "        [[ 0.91607137,  0.57395644, -0.90914179,  0.97212912,\n",
        "           0.33078759]]]])\n",
        "    query = query.float().transpose(1,2)\n",
        "    key = key.float().transpose(1,2)\n",
        "    value = value.float().transpose(1,2)\n",
        "\n",
        "    x,_ = torch.max(query[:,0,:,:], axis=-1)\n",
        "    mask = compositional_mask(x)\n",
        "    mask.unsqueeze_(1)\n",
        "    for n,t in [('query', query), ('key', key), ('value', value), ('mask', mask)]:\n",
        "        print(f'Name: {n}, shape: {t.size()}')\n",
        "    with torch.no_grad():\n",
        "        output, attn_weights = mha.attention(query, key, value, mask=mask)\n",
        "    assert output.size() == torch.Size([2,1,3,5])\n",
        "    assert attn_weights.size() == torch.Size([2,1,3,3])\n",
        "\n",
        "    truth_output = torch.tensor([[[[-0.8390,  0.8050,  0.2230, -0.8485, -0.3491],\n",
        "          [-0.6043,  0.5212,  0.1076, -0.8146, -0.2870],\n",
        "          [-0.0665,  0.2461,  0.3038, -0.7137, -0.4410]]],\n",
        "        [[[-0.4783,  0.3446, -0.4192,  0.3387,  0.4347],\n",
        "          [-0.7959,  0.1942, -0.4652,  0.7239,  0.1769],\n",
        "          [-0.3678,  0.2868, -0.5799,  0.7987,  0.2086]]]])\n",
        "    truth_attn_weights = torch.tensor([[[[1.0000, 0.0000, 0.0000],\n",
        "          [0.7103, 0.2897, 0.0000],\n",
        "          [0.3621, 0.3105, 0.3274]]],\n",
        "        [[[1.0000, 0.0000, 0.0000],\n",
        "          [0.3793, 0.6207, 0.0000],\n",
        "          [0.2642, 0.4803, 0.2555]]]])\n",
        "    # print(torch.abs(output - truth_output))\n",
        "    # print(torch.abs(attn_weights - truth_attn_weights))\n",
        "    assert torch.all(torch.abs(output - truth_output) < 1e-4).item()\n",
        "    assert torch.all(torch.abs(attn_weights - truth_attn_weights) < 1e-4).item()\n",
        "    print('Test is passed!')"
      ],
      "metadata": {
        "id": "ExHkza22FCF0",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:14:58.694673Z",
          "iopub.execute_input": "2023-04-11T19:14:58.695743Z",
          "iopub.status.idle": "2023-04-11T19:14:58.715024Z",
          "shell.execute_reply.started": "2023-04-11T19:14:58.695698Z",
          "shell.execute_reply": "2023-04-11T19:14:58.713934Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_multi_head_attention()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GI6yMbX8FhGZ",
        "outputId": "a97bb69c-f4ce-4095-b672-4accc8c279e1",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:14:58.716674Z",
          "iopub.execute_input": "2023-04-11T19:14:58.717069Z",
          "iopub.status.idle": "2023-04-11T19:14:58.755145Z",
          "shell.execute_reply.started": "2023-04-11T19:14:58.717030Z",
          "shell.execute_reply": "2023-04-11T19:14:58.754115Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Name: query, shape: torch.Size([2, 1, 3, 5])\nName: key, shape: torch.Size([2, 1, 3, 5])\nName: value, shape: torch.Size([2, 1, 3, 5])\nName: mask, shape: torch.Size([2, 1, 3, 3])\nTest is passed!\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "LUdLLSojGJbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
        "\n",
        "    def __init__(self, hidden_size, ff_hidden_size, n_heads, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(n_heads, hidden_size,\n",
        "                                            dropout=dropout['attention'])\n",
        "        self.feed_forward = FeedForward(hidden_size, ff_hidden_size,\n",
        "                                        dropout=dropout['relu'])\n",
        "        self.sublayers = clone_layer(SublayerConnection(hidden_size, dropout['residual']), 2)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        return self.sublayers[1](x, self.feed_forward)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedder = Embedding(config['hidden_size'],\n",
        "                                  config['src_vocab_size'])\n",
        "        self.positional_encoder = PositionalEncoding(config['hidden_size'],\n",
        "                                                     max_len=config['max_src_seq_length'])\n",
        "        self.embedding_dropout = nn.Dropout(p=config['dropout']['embedding'])\n",
        "        self.encoder_layer = EncoderLayer(config['hidden_size'],\n",
        "                                          config['ff_hidden_size'],\n",
        "                                          config['n_heads'],\n",
        "                                          config['dropout'])\n",
        "        self.layers = clone_layer(self.encoder_layer, config['n_layers'])\n",
        "        self.layer_norm = LayerNorm(config['hidden_size'])\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        x = self.embedding_dropout(self.positional_encoder(self.embedder(x)))\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.layer_norm(x)"
      ],
      "metadata": {
        "id": "dFMMTX4NA0KP",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:14:58.756641Z",
          "iopub.execute_input": "2023-04-11T19:14:58.756988Z",
          "iopub.status.idle": "2023-04-11T19:14:58.767360Z",
          "shell.execute_reply.started": "2023-04-11T19:14:58.756953Z",
          "shell.execute_reply": "2023-04-11T19:14:58.766291Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "6kyeSkMeGQo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder is made of 3 sublayers: self attention, encoder-decoder attention\n",
        "    and feed forward\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size, ff_hidden_size, n_heads, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(n_heads, hidden_size,\n",
        "                                            dropout=dropout['attention'])\n",
        "        self.encdec_attn = MultiHeadAttention(n_heads, hidden_size,\n",
        "                                              dropout=dropout['attention'])\n",
        "        self.feed_forward = FeedForward(hidden_size, ff_hidden_size,\n",
        "                                        dropout=dropout['relu'])\n",
        "        self.sublayers = clone_layer(SublayerConnection(hidden_size, dropout['residual']), 3)\n",
        "\n",
        "    def forward(self, x, encoder_output, encoder_mask, decoder_mask):\n",
        "        x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, decoder_mask))\n",
        "        x = self.sublayers[1](x, lambda x: self.encdec_attn(x, encoder_output,\n",
        "                                                            encoder_output, encoder_mask))\n",
        "        return self.sublayers[2](x, self.feed_forward)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedder = Embedding(config['hidden_size'],\n",
        "                                  config['tgt_vocab_size'])\n",
        "        self.positional_encoder = PositionalEncoding(config['hidden_size'],\n",
        "                                                     max_len=config['max_tgt_seq_length'])\n",
        "        self.embedding_dropout = nn.Dropout(p=config['dropout']['embedding'])\n",
        "        self.decoder_layer = DecoderLayer(config['hidden_size'],\n",
        "                                          config['ff_hidden_size'],\n",
        "                                          config['n_heads'],\n",
        "                                          config['dropout'])\n",
        "        self.layers = clone_layer(self.decoder_layer, config['n_layers'])\n",
        "        self.layer_norm = LayerNorm(config['hidden_size'])\n",
        "\n",
        "    def forward(self, x, encoder_output, encoder_mask, decoder_mask):\n",
        "        x = self.embedding_dropout(self.positional_encoder(self.embedder(x)))\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, encoder_mask, decoder_mask)\n",
        "        return self.layer_norm(x)"
      ],
      "metadata": {
        "id": "B4pSnS8NGPyf",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:14:58.768953Z",
          "iopub.execute_input": "2023-04-11T19:14:58.769627Z",
          "iopub.status.idle": "2023-04-11T19:14:58.782810Z",
          "shell.execute_reply.started": "2023-04-11T19:14:58.769587Z",
          "shell.execute_reply": "2023-04-11T19:14:58.781826Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer"
      ],
      "metadata": {
        "id": "bwP_NVeYGY-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.config = config\n",
        "        self.encoder = Encoder(config)\n",
        "        self.decoder = Decoder(config)\n",
        "        self.proj = nn.Linear(config['hidden_size'], config['tgt_vocab_size'])\n",
        "\n",
        "        self.pad_idx = config['pad_idx']\n",
        "        self.tgt_vocab_size = config['tgt_vocab_size']\n",
        "\n",
        "    def encode(self, encoder_input, encoder_input_mask):\n",
        "        return self.encoder(encoder_input, encoder_input_mask)\n",
        "\n",
        "    def decode(self, encoder_output, encoder_input_mask, decoder_input, decoder_input_mask):\n",
        "        return self.decoder(decoder_input, encoder_output, encoder_input_mask, decoder_input_mask)\n",
        "\n",
        "    def linear_project(self, x):\n",
        "        return self.proj(x)\n",
        "\n",
        "    def forward(self, encoder_input, decoder_input):\n",
        "        encoder_input_mask = padding_mask(encoder_input, pad_idx=self.config['pad_idx'])\n",
        "        decoder_input_mask = compositional_mask(decoder_input, pad_idx=self.config['pad_idx'])\n",
        "        encoder_output = self.encode(encoder_input, encoder_input_mask)\n",
        "        decoder_output = self.decode(encoder_output, encoder_input_mask,\n",
        "                                     decoder_input, decoder_input_mask)\n",
        "        output_logits = self.linear_project(decoder_output)\n",
        "        return output_logits\n",
        "\n",
        "\n",
        "def prepare_model(config):\n",
        "    model = Transformer(config)\n",
        "\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "    return model"
      ],
      "metadata": {
        "id": "mVTRK_5cGYYa",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:14:58.784358Z",
          "iopub.execute_input": "2023-04-11T19:14:58.784724Z",
          "iopub.status.idle": "2023-04-11T19:14:58.796761Z",
          "shell.execute_reply.started": "2023-04-11T19:14:58.784689Z",
          "shell.execute_reply": "2023-04-11T19:14:58.795617Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  LrScheduler"
      ],
      "metadata": {
        "id": "wnmPBcVyrR6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last thing you have to prepare is the class  `LrScheduler`, which is in charge of  learning rate updating after every step of the optimizer. You are required to fill the class constructor and the method `learning_rate`. The preferable stratagy of updating the learning rate (lr), is the following two stages:\n",
        "\n",
        "* \"warmup\" stage - lr linearly increases until the defined value during the fixed number of steps (the proportion of all training steps - the parameter `train_config['warmup\\_steps\\_part']` in the train function). \n",
        "* \"decrease\" stage - lr linearly decreases until 0 during the left training steps.\n",
        "\n",
        "`learning_rate()` call should return the value of  lr at this step,  which number is stored at self.step. The class constructor takes not only `warmup_steps_part` but the peak learning rate value `lr_peak` at the end of \"warmup\" stage and a string name of the strategy of learning rate scheduling. You can test other strategies if you want to with `self.type attribute`. \n",
        "\n",
        "Correctness check: `test_lr_scheduler()`\n"
      ],
      "metadata": {
        "id": "2luuBDZFrTj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LrScheduler:\n",
        "    def __init__(self, n_steps, **kwargs):\n",
        "        self.type = kwargs['type']\n",
        "        if self.type == 'warmup,decay_linear':\n",
        "            ## TODO: provide your implementation here\n",
        "            self.lr_peak = kwargs['lr_peak']\n",
        "            self.warmup_steps = kwargs['warmup_steps_part'] * n_steps\n",
        "            self.decay_steps = (1 - kwargs['warmup_steps_part']) * n_steps\n",
        "            self.n_steps = n_steps\n",
        "        else:\n",
        "            raise ValueError(f'Unknown type argument: {self.type}')\n",
        "        self._step = 0\n",
        "        self._lr = 0\n",
        "\n",
        "    def step(self, optimizer):\n",
        "        self._step += 1\n",
        "        lr = self.learning_rate()\n",
        "        for p in optimizer.param_groups:\n",
        "            p['lr'] = lr\n",
        "\n",
        "    def learning_rate(self, step=None):\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        if self.type == 'warmup,decay_linear':\n",
        "            ## TODO: provide your implementation here\n",
        "            if step <= self.warmup_steps:\n",
        "                incr = self.lr_peak / self.warmup_steps\n",
        "                self._lr = incr * step\n",
        "            else:\n",
        "                decr = self.lr_peak / self.decay_steps\n",
        "                self._lr = self.lr_peak - decr * (step - self.warmup_steps)\n",
        "        return self._lr\n",
        "\n",
        "    def state_dict(self):\n",
        "        sd = copy.deepcopy(self.__dict__)\n",
        "        return sd\n",
        "\n",
        "    def load_state_dict(self, sd):\n",
        "        for k in sd.keys():\n",
        "            self.__setattr__(k, sd[k])"
      ],
      "metadata": {
        "id": "YDvKYF5EAdnX",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:14:58.799851Z",
          "iopub.execute_input": "2023-04-11T19:14:58.800103Z",
          "iopub.status.idle": "2023-04-11T19:14:58.811260Z",
          "shell.execute_reply.started": "2023-04-11T19:14:58.800079Z",
          "shell.execute_reply": "2023-04-11T19:14:58.810282Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_lr_scheduler():\n",
        "    lrs_type = 'warmup,decay_linear'\n",
        "    warmup_steps_part =  0.1\n",
        "    lr_peak = 3e-4\n",
        "    sch = LrScheduler(100, type=lrs_type, warmup_steps_part=warmup_steps_part,\n",
        "                      lr_peak=lr_peak)\n",
        "    assert sch.learning_rate(step=5) - 15e-5 < 1e-6\n",
        "    assert sch.learning_rate(step=10) - 3e-4 < 1e-6\n",
        "    assert sch.learning_rate(step=50) - 166e-6 < 1e-6\n",
        "    assert sch.learning_rate(step=100) - 0. < 1e-6\n",
        "    print('Test is passed!')"
      ],
      "metadata": {
        "id": "4JHOgJDBGjhr",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:14:58.812962Z",
          "iopub.execute_input": "2023-04-11T19:14:58.813342Z",
          "iopub.status.idle": "2023-04-11T19:14:58.824649Z",
          "shell.execute_reply.started": "2023-04-11T19:14:58.813307Z",
          "shell.execute_reply": "2023-04-11T19:14:58.823684Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_lr_scheduler()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2Ys4DZRGmzM",
        "outputId": "3ef9b88f-be99-4047-a092-cb1c27836268",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:14:58.830171Z",
          "iopub.execute_input": "2023-04-11T19:14:58.830434Z",
          "iopub.status.idle": "2023-04-11T19:14:58.835754Z",
          "shell.execute_reply.started": "2023-04-11T19:14:58.830407Z",
          "shell.execute_reply": "2023-04-11T19:14:58.834626Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Test is passed!\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run and translate"
      ],
      "metadata": {
        "id": "byCY6Tn-A9i_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "\n",
        "def run_epoch(data_iter, model, lr_scheduler, optimizer, device, verbose=False):\n",
        "    start = time.time()\n",
        "    local_start = start\n",
        "    total_tokens = 0\n",
        "    total_loss = 0\n",
        "    tokens = 0\n",
        "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
        "    for i, batch in tqdm(enumerate(data_iter)):\n",
        "        encoder_input = batch[0].to(device)\n",
        "        decoder_input = batch[1].to(device)\n",
        "        decoder_target = batch[2].to(device)\n",
        "        logits = model(encoder_input, decoder_input)\n",
        "        loss = loss_fn(logits.view(-1, model.tgt_vocab_size),\n",
        "                       decoder_target.view(-1))\n",
        "        total_loss += loss.item()\n",
        "        batch_n_tokens = (decoder_target != model.pad_idx).sum().item()\n",
        "        total_tokens += batch_n_tokens\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "            lr_scheduler.step(optimizer)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        tokens += batch_n_tokens\n",
        "        if verbose and i % 1000 == 1:\n",
        "            elapsed = time.time() - local_start\n",
        "            if epoch % 10 == 0:\n",
        "                print(\"batch number: %d, accumulated average loss: %f, tokens per second: %f\" %\n",
        "                      (i, total_loss / total_tokens, tokens / elapsed))\n",
        "            local_start = time.time()\n",
        "            tokens = 0\n",
        "\n",
        "    average_loss = total_loss / total_tokens\n",
        "    print('** End of epoch, accumulated average loss = %f **' % average_loss)\n",
        "    epoch_elapsed_time = format_time(time.time() - start)\n",
        "    return average_loss\n",
        "\n",
        "\n",
        "def save_checkpoint(epoch, model, lr_scheduler, optimizer, model_dir_path):\n",
        "    save_path = os.path.join(model_dir_path,f'cpkt_{epoch}_epoch')\n",
        "    save_path = model_dir_path + f'epoch_{epoch}'\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'lr_scheduler_state_dict': lr_scheduler.state_dict()\n",
        "    }, save_path)\n",
        "    wandb.save(save_path)\n",
        "    print(f'Saved checkpoint to {save_path}')\n",
        "\n",
        "def load_model(epoch, model_dir_path):\n",
        "    save_path = os.path.join(model_dir_path, f'cpkt_{epoch}_epoch')\n",
        "    checkpoint = torch.load(save_path)\n",
        "    with open(os.path.join(model_dir_path, 'model_config.json'), 'r', encoding='utf-8') as rf:\n",
        "        model_config = json.load(rf)\n",
        "    model = prepare_model(model_config)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    return model\n",
        "\n",
        "def greedy_decode(model, device, encoder_input, max_len, start_symbol):\n",
        "    batch_size = encoder_input.size()[0]\n",
        "    decoder_input = torch.ones(batch_size, 1).fill_(start_symbol).type_as(encoder_input.data).to(device)\n",
        "\n",
        "    for i in range(max_len):\n",
        "        logits = model(encoder_input, decoder_input)\n",
        "\n",
        "        _, predicted_ids = torch.max(logits, dim=-1)\n",
        "        next_word = predicted_ids[:, i]\n",
        "        # print(next_word)\n",
        "        rest = torch.ones(batch_size, 1).type_as(decoder_input.data)\n",
        "        # print(rest[:,0].size(), next_word.size())\n",
        "        rest[:, 0] = next_word\n",
        "        decoder_input = torch.cat([decoder_input, rest], dim=1).to(device)\n",
        "        # print(decoder_input)\n",
        "    return decoder_input\n",
        "\n",
        "def generate_predictions(dataloader, max_decoding_len, text_encoder, model, device):\n",
        "    # print(f'Max decoding length = {max_decoding_len}')\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    start_token_id = text_encoder.service_vocabs['token2id'][\n",
        "        text_encoder.service_token_names['start_token']]\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            encoder_input = batch[0].to(device)\n",
        "            prediction_tensor = \\\n",
        "                greedy_decode(model, device, encoder_input, max_decoding_len,\n",
        "                              start_token_id)\n",
        "\n",
        "            predictions.extend([''.join(e) for e in text_encoder.id2token(prediction_tensor.cpu().numpy(),\n",
        "                                                                          unframe=True, lang_key='ru')])\n",
        "    return np.array(predictions)\n",
        "\n",
        "\n",
        "def train(source_strings, target_strings, tune_params = None):\n",
        "    '''Common training cycle for final run (fixed hyperparameters,\n",
        "    no evaluation during training)'''\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f'Using GPU device: {device}')\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(f'GPU is not available, using CPU device {device}')\n",
        "\n",
        "    train_df = pd.DataFrame({'en': source_strings, 'ru': target_strings})\n",
        "    text_encoder = TextEncoder()\n",
        "    text_encoder.make_vocabs(train_df)\n",
        "    model_config = {\n",
        "        'src_vocab_size': text_encoder.src_vocab_size,\n",
        "        'tgt_vocab_size': text_encoder.tgt_vocab_size,\n",
        "        'max_src_seq_length': max(train_df['en'].aggregate(len)) + 2, #including start_token and end_token\n",
        "        'max_tgt_seq_length': max(train_df['ru'].aggregate(len)) + 2,\n",
        "        'n_layers': 2,\n",
        "        'n_heads': 2,\n",
        "        'hidden_size': 128,\n",
        "        'ff_hidden_size': 256,\n",
        "        'dropout': {\n",
        "            'embedding': tune_params['embedding'] if tune_params else 0.1,\n",
        "            'attention': tune_params['attention'] if tune_params else 0.1,\n",
        "            'residual': tune_params['residual'] if tune_params else 0.1,\n",
        "            'relu': tune_params['relu'] if tune_params else 0.1\n",
        "        },\n",
        "        'pad_idx': 0\n",
        "    }\n",
        "    \n",
        "    wandb.init(project=\"hw1-NLP\", \n",
        "           group=\"Bamblbi\",\n",
        "           config=model_config)\n",
        "    \n",
        "    model = prepare_model(model_config)\n",
        "    model.to(device)\n",
        "\n",
        "    train_config = {'batch_size': 200, 'n_epochs': 300, 'lr_scheduler': {\n",
        "        'type': 'warmup,decay_linear',\n",
        "        'warmup_steps_part': 0.1,\n",
        "        'lr_peak': tune_params['lr_peak'] if tune_params else 3e-4,\n",
        "    }}\n",
        "\n",
        "    #Model training procedure\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.)\n",
        "    n_steps = (len(train_df) // train_config['batch_size'] + 1) * train_config['n_epochs']\n",
        "    lr_scheduler = LrScheduler(n_steps, **train_config['lr_scheduler'])\n",
        "\n",
        "    # prepare train data\n",
        "    source_strings, target_strings = zip(*sorted(zip(source_strings, target_strings),\n",
        "                                                 key=lambda e: len(e[0])))\n",
        "    train_dataloader = create_dataloader(source_strings, target_strings, text_encoder,\n",
        "                                         train_config['batch_size'],\n",
        "                                         shuffle_batches_each_epoch=True)\n",
        "    # training cycle\n",
        "    for epoch in range(1,train_config['n_epochs']+1):\n",
        "        print('\\n' + '-'*40)\n",
        "        print(f'Epoch: {epoch}')\n",
        "        print(f'Run training...')\n",
        "        model.train()\n",
        "        av_loss = run_epoch(train_dataloader, model,\n",
        "                          lr_scheduler, optimizer, device=device, verbose=False)\n",
        "        wandb.log({'loss_train': av_loss})\n",
        "        if epoch % 20 == 0:\n",
        "            save_checkpoint(epoch, model, lr_scheduler, optimizer, f'bamblbi_')\n",
        "\n",
        "    learnable_params = {\n",
        "        'model': model,\n",
        "        'text_encoder': text_encoder,\n",
        "    }\n",
        "    return learnable_params\n",
        "\n",
        "def classify(source_strings, learnable_params):\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f'Using GPU device: {device}')\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(f'GPU is not available, using CPU device {device}')\n",
        "\n",
        "    model = learnable_params['model']\n",
        "    text_encoder = learnable_params['text_encoder']\n",
        "    batch_size = 200\n",
        "    dataloader = create_dataloader(source_strings, None, text_encoder,\n",
        "                                   batch_size, shuffle_batches_each_epoch=False)\n",
        "    max_decoding_len = model.config['max_tgt_seq_length']\n",
        "    predictions = generate_predictions(dataloader, max_decoding_len, text_encoder, model, device)\n",
        "    #return single top1 prediction for each sample\n",
        "    return np.expand_dims(predictions, 1)"
      ],
      "metadata": {
        "id": "-K7-KJEGA8po",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:14:58.837895Z",
          "iopub.execute_input": "2023-04-11T19:14:58.838297Z",
          "iopub.status.idle": "2023-04-11T19:14:58.870608Z",
          "shell.execute_reply.started": "2023-04-11T19:14:58.838262Z",
          "shell.execute_reply": "2023-04-11T19:14:58.869619Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "jpG6i8X-HMmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PREDS_FNAME = \"preds_translit_bl_0.tsv\"\n",
        "SCORED_PARTS = ('train', 'dev', 'train_small', 'dev_small', 'test')\n",
        "TRANSLIT_PATH = \"TRANSLIT\""
      ],
      "metadata": {
        "id": "f-7-YtzEKnug",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:14:58.871891Z",
          "iopub.execute_input": "2023-04-11T19:14:58.872324Z",
          "iopub.status.idle": "2023-04-11T19:14:58.883734Z",
          "shell.execute_reply.started": "2023-04-11T19:14:58.872288Z",
          "shell.execute_reply": "2023-04-11T19:14:58.882748Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install wandb -qqq"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-11T19:15:06.055539Z",
          "iopub.execute_input": "2023-04-11T19:15:06.056158Z",
          "iopub.status.idle": "2023-04-11T19:15:15.444831Z",
          "shell.execute_reply.started": "2023-04-11T19:15:06.056117Z",
          "shell.execute_reply": "2023-04-11T19:15:15.443413Z"
        },
        "trusted": true,
        "id": "ogC2VwNhG82R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-11T19:15:15.447640Z",
          "iopub.execute_input": "2023-04-11T19:15:15.448048Z",
          "iopub.status.idle": "2023-04-11T19:15:15.955440Z",
          "shell.execute_reply.started": "2023-04-11T19:15:15.447999Z",
          "shell.execute_reply": "2023-04-11T19:15:15.954248Z"
        },
        "trusted": true,
        "id": "aeo2_0u9G82S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 1\n",
        "part2ixy = load_dataset(TRANSLIT_PATH, parts=SCORED_PARTS)\n",
        "train_ids, train_strings, train_transliterations = part2ixy['train']\n",
        "print('\\nTraining classifier on %d examples from train set ...' % len(train_strings))\n",
        "st = time.time()\n",
        "params = train(train_strings, train_transliterations)\n",
        "print('Classifier trained in %.2fs' % (time.time() - st))"
      ],
      "metadata": {
        "id": "74GcLUTuLFyS",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allpreds = []\n",
        "for part, (ids, x, y) in part2ixy.items():\n",
        "    print('\\nClassifying %s set with %d examples ...' % (part, len(x)))\n",
        "    st = time.time()\n",
        "    preds = classify(x, params)\n",
        "    print('%s set classified in %.2fs' % (part, time.time() - st))\n",
        "    count_of_values = list(map(len, preds))\n",
        "    assert np.all(np.array(count_of_values) == top_k)\n",
        "    #score(preds, y)\n",
        "    allpreds.extend(zip(ids, preds))\n",
        "\n",
        "save_preds(allpreds, preds_fname=PREDS_FNAME)\n",
        "print('\\nChecking saved predictions ...')\n",
        "score_preds(preds_path=PREDS_FNAME, data_dir=TRANSLIT_PATH, parts=SCORED_PARTS)"
      ],
      "metadata": {
        "id": "hPELZcXeHLHF",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Hyper-parameters choice"
      ],
      "metadata": {
        "id": "oFfDH0-SsRm-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is ready. Now we need to find the optimal hyper-parameters.\n",
        "\n",
        "The quality of models with different hyperparameters should be monitored on dev or on dev_small samples (in order to save time, since generating transliterations is a rather time-consuming process, comparable to one training epoch).\n",
        "\n",
        "To generate predictions, you can use the `generate_predictions` function, to calculate the accuracy@1 metric, and then you can use the `compute_metrics` function.\n",
        "\n",
        "\n",
        "\n",
        "Hyper-parameters are stored in the dictionary `model_config` and `train_config` in train function. The following hyperparameters in `model_config` and `train_config` are suggested to leave unmodified:\n",
        "\n",
        "* n_layers $=$ 2\n",
        "* n_heads $=$ 2\n",
        "* hidden_size $=$ 128\n",
        "* fc_hidden_size $=$ 256\n",
        "* warmup_steps_part $=$ 0.1\n",
        "* batch_size $=$ 200\n",
        "\n",
        " You can vary the dropout value. The model has 4 types of : ***embedding dropout*** applied on embdeddings before sending to the first layer of  Encoder or Decoder, ***attention*** dropout applied on the attention weights in the MultiHeadAttention layer, ***residual dropout*** applied on the output of each sublayer (MultiHeadAttention or FeedForward) in layers Encoder and Decoder and, finaly, ***relu dropout*** in used in FeedForward layer. For all 4 types it is suggested to test the same value of dropout from the list: 0.1, 0.15, 0.2.\n",
        " Also it is suggested to test several peak levels of learning rate - **lr_peak** : 5e-4, 1e-3, 2e-3.\n",
        "\n",
        "Note that if you are using a GPU, then training one epoch takes about 1 minute, and up to 1 GB of video memory is required. When using the CPU, the learning speed slows down by about 2 times. If there are problems with insufficient RAM / video memory, reduce the batch size, but in this case the optimal range of learning rate values will change, and it must be determined again. To train a model with  batch_size $=$ 200 , it will take at least 300 epochs to achieve accuracy 0.66 on dev_small dataset."
      ],
      "metadata": {
        "id": "PxqZbEmtsV0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Question: What are the optimal hyperpameters according to your experiments? Add plots or other descriptions here.* \n",
        "\n",
        "```\n",
        "I had got my best score on the leaderboard (0.66) just by training model in 600 epochs with initial parameters which was given in this notebook.\n",
        "\n",
        "And I didn't have time to choose best hyperparameters. But here is the code that could get it.\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "IQXVmzk0a60Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are also some loss plots. But it is too early to make any conclusions:"
      ],
      "metadata": {
        "id": "7DpYav3qG82U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!pip install optuna-dashboard\n",
        "\n",
        "!optuna-dashboard sqlite:///example-study.db"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-11T19:15:15.959312Z",
          "iopub.execute_input": "2023-04-11T19:15:15.959631Z",
          "iopub.status.idle": "2023-04-11T19:15:29.581280Z",
          "shell.execute_reply.started": "2023-04-11T19:15:15.959597Z",
          "shell.execute_reply": "2023-04-11T19:15:29.579919Z"
        },
        "trusted": true,
        "id": "aKIyVwWQG82U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "from optuna.visualization import plot_contour\n",
        "from optuna.visualization import plot_edf\n",
        "from optuna.visualization import plot_intermediate_values\n",
        "from optuna.visualization import plot_optimization_history\n",
        "from optuna.visualization import plot_parallel_coordinate\n",
        "from optuna.visualization import plot_param_importances\n",
        "from optuna.visualization import plot_slice\n",
        "\n",
        "SEED = 1337\n",
        "\n",
        "np.random.seed(SEED)"
      ],
      "metadata": {
        "id": "ylE7wnrXJqmq",
        "execution": {
          "iopub.status.busy": "2023-04-11T19:15:29.584230Z",
          "iopub.execute_input": "2023-04-11T19:15:29.584652Z",
          "iopub.status.idle": "2023-04-11T19:15:30.325912Z",
          "shell.execute_reply.started": "2023-04-11T19:15:29.584607Z",
          "shell.execute_reply": "2023-04-11T19:15:30.324840Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    SCORED_PARTS = ('train', 'dev', 'train_small', 'dev_small', 'test')\n",
        "\n",
        "    tune_params = {\n",
        "                'embedding': trial.suggest_categorical('embedding', [0.1, 0.15, 0.2]),\n",
        "                'attention': trial.suggest_categorical('attention', [0.1, 0.15, 0.2]),\n",
        "                'residual': trial.suggest_categorical('residual', [0.1, 0.15, 0.2]),\n",
        "                'relu': trial.suggest_categorical('relu', [0.1, 0.15, 0.2]),\n",
        "                'lr_peak': trial.suggest_categorical('lr_peak', [5e-4, 1e-3, 2e-3])\n",
        "            }\n",
        "    top_k = 1\n",
        "    part2ixy = load_dataset(TRANSLIT_PATH, parts=SCORED_PARTS)\n",
        "    train_ids, train_strings, train_transliterations = part2ixy['train']\n",
        "    source_strings, target_strings = train_strings, train_transliterations\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f'Using GPU device: {device}')\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(f'GPU is not available, using CPU device {device}')\n",
        "\n",
        "    train_df = pd.DataFrame({'en': source_strings, 'ru': target_strings})\n",
        "    text_encoder = TextEncoder()\n",
        "    text_encoder.make_vocabs(train_df)\n",
        "    model_config = {\n",
        "        'src_vocab_size': text_encoder.src_vocab_size,\n",
        "        'tgt_vocab_size': text_encoder.tgt_vocab_size,\n",
        "        'max_src_seq_length': max(train_df['en'].aggregate(len)) + 2, #including start_token and end_token\n",
        "        'max_tgt_seq_length': max(train_df['ru'].aggregate(len)) + 2,\n",
        "        'n_layers': 2,\n",
        "        'n_heads': 2,\n",
        "        'hidden_size': 128,\n",
        "        'ff_hidden_size': 256,\n",
        "        'dropout': {\n",
        "            'embedding': tune_params['embedding'] if tune_params else 0.1,\n",
        "            'attention': tune_params['attention'] if tune_params else 0.1,\n",
        "            'residual': tune_params['residual'] if tune_params else 0.1,\n",
        "            'relu': tune_params['relu'] if tune_params else 0.1\n",
        "        },\n",
        "        'pad_idx': 0\n",
        "    }\n",
        "    \n",
        "    wandb.init(project=\"hw1-NLP\", \n",
        "               name=f\"preds_translit_bl_emb_{tune_params['embedding']}_att_{tune_params['attention']}\"\n",
        "                           +f\"res_{tune_params['residual']}_relu_{tune_params['relu']}_lr_{tune_params['lr_peak']}\",\n",
        "               group=\"Bamblbi_finetuning_new\",\n",
        "               config=model_config)\n",
        "    \n",
        "    model = prepare_model(model_config)\n",
        "    model.to(device)\n",
        "\n",
        "    train_config = {'batch_size': 200, 'n_epochs': 100, 'lr_scheduler': {\n",
        "        'type': 'warmup,decay_linear',\n",
        "        'warmup_steps_part': 0.1,\n",
        "        'lr_peak': tune_params['lr_peak'] if tune_params else 3e-4,\n",
        "    }}\n",
        "\n",
        "    #Model training procedure\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.)\n",
        "    n_steps = (len(train_df) // train_config['batch_size'] + 1) * train_config['n_epochs']\n",
        "    lr_scheduler = LrScheduler(n_steps, **train_config['lr_scheduler'])\n",
        "\n",
        "    # prepare train data\n",
        "    source_strings, target_strings = zip(*sorted(zip(source_strings, target_strings),\n",
        "                                                 key=lambda e: len(e[0])))\n",
        "    train_dataloader = create_dataloader(source_strings, target_strings, text_encoder,\n",
        "                                         train_config['batch_size'],\n",
        "                                         shuffle_batches_each_epoch=True)\n",
        "    # training cycle\n",
        "    for epoch in range(1,train_config['n_epochs']+1):\n",
        "            print('\\n' + '-'*40)\n",
        "            print(f'Epoch: {epoch}')\n",
        "            print(f'Run training...')\n",
        "            model.train()\n",
        "            av_loss = run_epoch(train_dataloader, model,\n",
        "                              lr_scheduler, optimizer, device=device, verbose=False)\n",
        "            wandb.log({'loss_train': av_loss})\n",
        "            if epoch % 20 == 0:\n",
        "                save_checkpoint(epoch, model, lr_scheduler, optimizer, f'bamblbi_f')\n",
        "\n",
        "            params = {\n",
        "            'model': model,\n",
        "            'text_encoder': text_encoder,\n",
        "             }\n",
        "    allpreds = []\n",
        "    for part, (ids, x, y) in part2ixy.items():\n",
        "        if part == 'dev' or part == 'dev_small':\n",
        "            st = time.time()\n",
        "            preds = classify(x, params)\n",
        "            count_of_values = list(map(len, preds))\n",
        "            assert np.all(np.array(count_of_values) == top_k)\n",
        "            #score(preds, y)\n",
        "            allpreds.extend(zip(ids, preds))\n",
        "    SCORED_PARTS = ('dev', 'dev_small')\n",
        "    PREDS_FNAME = f\"preds_translit_bl_f{tune_params['embedding']}_{tune_params['attention']}_{tune_params['residual']}_{tune_params['relu']}_{tune_params['lr_peak']}.tsv\"\n",
        "    save_preds(allpreds, preds_fname=PREDS_FNAME)\n",
        "    score = score_preds(preds_path=PREDS_FNAME, data_dir=TRANSLIT_PATH, parts=SCORED_PARTS)\n",
        "    return score['dev']['acc@1']\n",
        "\n",
        "     "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-11T19:18:08.428827Z",
          "iopub.execute_input": "2023-04-11T19:18:08.429194Z",
          "iopub.status.idle": "2023-04-11T19:18:08.455881Z",
          "shell.execute_reply.started": "2023-04-11T19:18:08.429162Z",
          "shell.execute_reply": "2023-04-11T19:18:08.453347Z"
        },
        "trusted": true,
        "id": "apLxjqFDG82V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "o39yh3QAG82V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    sampler=optuna.samplers.TPESampler(seed=SEED),\n",
        ")\n",
        "study.optimize(objective, n_trials=8)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-11T19:18:10.477727Z",
          "iopub.execute_input": "2023-04-11T19:18:10.478795Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "86778b93ade14fb6b5709d22dbc915d6",
            "c67bd4241117428c9bcd3c9967a6cb7c",
            "265b3b31dd72496b806ee71fb1db36b1",
            "db657f730dff42718e34ad780e360617",
            "dceec8dd60c7434291da1914f19073c8"
          ]
        },
        "id": "0BVUKy8LG82V",
        "outputId": "627b77dc-f905-4142-9def-956d29e3dc55"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[32m[I 2023-04-11 19:18:10,487]\u001b[0m A new study created in memory with name: no-name-3ed3d218-dc4c-4e5c-9fc1-8ff073304dfd\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Using GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Finishing last run (ID:hhjnk8eu) before initializing another..."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='0.001 MB of 0.042 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.025709…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86778b93ade14fb6b5709d22dbc915d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss_train</td><td>█▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss_train</td><td>2.2568</td></tr></table><br/></div></div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">preds_translit_bl_emb_0.2_att_0.2res_{tune_params['residual']}_relu_{tune_params['relu']}_lr_{tune_params['lr_peak']}</strong> at: <a href='https://wandb.ai/eksolodneva/hw1-NLP/runs/hhjnk8eu' target=\"_blank\">https://wandb.ai/eksolodneva/hw1-NLP/runs/hhjnk8eu</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20230411_191636-hhjnk8eu/logs</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Successfully finished last run (ID:hhjnk8eu). Initializing new run:<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.14.2 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.14.0"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20230411_191811-7neakjv7</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/eksolodneva/hw1-NLP/runs/7neakjv7' target=\"_blank\">preds_translit_bl_emb_0.2_att_0.2res_0.15_relu_0.2_lr_0.001</a></strong> to <a href='https://wandb.ai/eksolodneva/hw1-NLP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/eksolodneva/hw1-NLP' target=\"_blank\">https://wandb.ai/eksolodneva/hw1-NLP</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/eksolodneva/hw1-NLP/runs/7neakjv7' target=\"_blank\">https://wandb.ai/eksolodneva/hw1-NLP/runs/7neakjv7</a>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n----------------------------------------\nEpoch: 1\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 31.74it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 4.207470 **\n\n----------------------------------------\nEpoch: 2\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.83it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 2.953771 **\n\n----------------------------------------\nEpoch: 3\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.19it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 2.102429 **\n\n----------------------------------------\nEpoch: 4\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.68it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 1.265408 **\n\n----------------------------------------\nEpoch: 5\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.24it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 1.014860 **\n\n----------------------------------------\nEpoch: 6\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.53it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.865737 **\n\n----------------------------------------\nEpoch: 7\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.07it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.738598 **\n\n----------------------------------------\nEpoch: 8\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.30it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.661671 **\n\n----------------------------------------\nEpoch: 9\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.76it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.618672 **\n\n----------------------------------------\nEpoch: 10\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.86it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.581310 **\n\n----------------------------------------\nEpoch: 11\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.38it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.555280 **\n\n----------------------------------------\nEpoch: 12\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.29it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.528049 **\n\n----------------------------------------\nEpoch: 13\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.18it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.508095 **\n\n----------------------------------------\nEpoch: 14\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.70it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.489967 **\n\n----------------------------------------\nEpoch: 15\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.17it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.475790 **\n\n----------------------------------------\nEpoch: 16\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.90it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.461721 **\n\n----------------------------------------\nEpoch: 17\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.98it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.451526 **\n\n----------------------------------------\nEpoch: 18\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 32.99it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.442905 **\n\n----------------------------------------\nEpoch: 19\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.83it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.432661 **\n\n----------------------------------------\nEpoch: 20\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.00it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.427170 **\nSaved checkpoint to bamblbi_fepoch_20\n\n----------------------------------------\nEpoch: 21\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.98it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.422271 **\n\n----------------------------------------\nEpoch: 22\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.93it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.414504 **\n\n----------------------------------------\nEpoch: 23\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.02it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.413119 **\n\n----------------------------------------\nEpoch: 24\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.88it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.406008 **\n\n----------------------------------------\nEpoch: 25\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.65it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.400984 **\n\n----------------------------------------\nEpoch: 26\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.11it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.399502 **\n\n----------------------------------------\nEpoch: 27\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.23it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.395590 **\n\n----------------------------------------\nEpoch: 28\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.06it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.391406 **\n\n----------------------------------------\nEpoch: 29\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.97it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.389312 **\n\n----------------------------------------\nEpoch: 30\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.23it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.385862 **\n\n----------------------------------------\nEpoch: 31\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.14it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.382567 **\n\n----------------------------------------\nEpoch: 32\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 32.96it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.380349 **\n\n----------------------------------------\nEpoch: 33\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.14it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.377040 **\n\n----------------------------------------\nEpoch: 34\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.92it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.376099 **\n\n----------------------------------------\nEpoch: 35\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.14it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.373625 **\n\n----------------------------------------\nEpoch: 36\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.24it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.371619 **\n\n----------------------------------------\nEpoch: 37\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.35it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.369341 **\n\n----------------------------------------\nEpoch: 38\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.17it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.367012 **\n\n----------------------------------------\nEpoch: 39\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.91it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.366295 **\n\n----------------------------------------\nEpoch: 40\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.02it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.363398 **\nSaved checkpoint to bamblbi_fepoch_40\n\n----------------------------------------\nEpoch: 41\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.39it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.362143 **\n\n----------------------------------------\nEpoch: 42\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 32.96it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.360022 **\n\n----------------------------------------\nEpoch: 43\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.74it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.358536 **\n\n----------------------------------------\nEpoch: 44\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.04it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.357044 **\n\n----------------------------------------\nEpoch: 45\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.82it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.356197 **\n\n----------------------------------------\nEpoch: 46\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.79it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.354834 **\n\n----------------------------------------\nEpoch: 47\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.81it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.351553 **\n\n----------------------------------------\nEpoch: 48\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.48it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.350657 **\n\n----------------------------------------\nEpoch: 49\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.12it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.350113 **\n\n----------------------------------------\nEpoch: 50\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.58it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.348711 **\n\n----------------------------------------\nEpoch: 51\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.88it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.347494 **\n\n----------------------------------------\nEpoch: 52\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.81it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.346574 **\n\n----------------------------------------\nEpoch: 53\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.27it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.345277 **\n\n----------------------------------------\nEpoch: 54\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.07it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.343887 **\n\n----------------------------------------\nEpoch: 55\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.73it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.342055 **\n\n----------------------------------------\nEpoch: 56\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.65it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.341018 **\n\n----------------------------------------\nEpoch: 57\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.62it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.339621 **\n\n----------------------------------------\nEpoch: 58\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.08it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.339104 **\n\n----------------------------------------\nEpoch: 59\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.25it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.337610 **\n\n----------------------------------------\nEpoch: 60\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.20it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.335890 **\nSaved checkpoint to bamblbi_fepoch_60\n\n----------------------------------------\nEpoch: 61\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.66it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.335895 **\n\n----------------------------------------\nEpoch: 62\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.85it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.334006 **\n\n----------------------------------------\nEpoch: 63\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.76it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.333520 **\n\n----------------------------------------\nEpoch: 64\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.12it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.332254 **\n\n----------------------------------------\nEpoch: 65\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.28it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.331492 **\n\n----------------------------------------\nEpoch: 66\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.70it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.329827 **\n\n----------------------------------------\nEpoch: 67\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.25it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.329267 **\n\n----------------------------------------\nEpoch: 68\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.37it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.327297 **\n\n----------------------------------------\nEpoch: 69\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.97it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.326864 **\n\n----------------------------------------\nEpoch: 70\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.37it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.327375 **\n\n----------------------------------------\nEpoch: 71\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.71it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.324503 **\n\n----------------------------------------\nEpoch: 72\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.10it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.324369 **\n\n----------------------------------------\nEpoch: 73\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.03it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.322751 **\n\n----------------------------------------\nEpoch: 74\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.85it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.322879 **\n\n----------------------------------------\nEpoch: 75\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.38it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.321650 **\n\n----------------------------------------\nEpoch: 76\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.62it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.320783 **\n\n----------------------------------------\nEpoch: 77\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.30it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.319761 **\n\n----------------------------------------\nEpoch: 78\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.05it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.319280 **\n\n----------------------------------------\nEpoch: 79\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.00it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.317638 **\n\n----------------------------------------\nEpoch: 80\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 32.99it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.317181 **\nSaved checkpoint to bamblbi_fepoch_80\n\n----------------------------------------\nEpoch: 81\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.30it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.316667 **\n\n----------------------------------------\nEpoch: 82\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.00it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.316174 **\n\n----------------------------------------\nEpoch: 83\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.96it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.314330 **\n\n----------------------------------------\nEpoch: 84\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.89it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.313323 **\n\n----------------------------------------\nEpoch: 85\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.15it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.314244 **\n\n----------------------------------------\nEpoch: 86\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.35it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.311951 **\n\n----------------------------------------\nEpoch: 87\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.63it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.311941 **\n\n----------------------------------------\nEpoch: 88\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.16it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.311273 **\n\n----------------------------------------\nEpoch: 89\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.28it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.309695 **\n\n----------------------------------------\nEpoch: 90\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.17it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.310241 **\n\n----------------------------------------\nEpoch: 91\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.28it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.308384 **\n\n----------------------------------------\nEpoch: 92\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.61it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.309103 **\n\n----------------------------------------\nEpoch: 93\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.80it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.308574 **\n\n----------------------------------------\nEpoch: 94\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.11it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.306948 **\n\n----------------------------------------\nEpoch: 95\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.90it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.307469 **\n\n----------------------------------------\nEpoch: 96\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.40it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.306775 **\n\n----------------------------------------\nEpoch: 97\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.77it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.305821 **\n\n----------------------------------------\nEpoch: 98\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.07it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.305666 **\n\n----------------------------------------\nEpoch: 99\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.97it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.305067 **\n\n----------------------------------------\nEpoch: 100\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.94it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.304366 **\nSaved checkpoint to bamblbi_fepoch_100\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:14<00:00,  9.06it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Using GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 10/10 [00:01<00:00,  7.30it/s]\n\u001b[32m[I 2023-04-11 19:45:22,090]\u001b[0m Trial 0 finished with value: 0.633095436944803 and parameters: {'embedding': 0.2, 'attention': 0.2, 'residual': 0.15, 'relu': 0.2, 'lr_peak': 0.001}. Best is trial 0 with value: 0.633095436944803.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Predictions saved to preds_translit_bl_f0.2_0.2_0.15_0.2_0.001.tsv\ndev set accuracy@1: 0.63\ndev_small set accuracy@1: 0.65\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Finishing last run (ID:7neakjv7) before initializing another..."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='47.204 MB of 47.243 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.9991…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c67bd4241117428c9bcd3c9967a6cb7c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss_train</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss_train</td><td>0.30437</td></tr></table><br/></div></div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">preds_translit_bl_emb_0.2_att_0.2res_0.15_relu_0.2_lr_0.001</strong> at: <a href='https://wandb.ai/eksolodneva/hw1-NLP/runs/7neakjv7' target=\"_blank\">https://wandb.ai/eksolodneva/hw1-NLP/runs/7neakjv7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 5 other file(s)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20230411_191811-7neakjv7/logs</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Successfully finished last run (ID:7neakjv7). Initializing new run:<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.14.2 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.14.0"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20230411_194523-yfpvutq0</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/eksolodneva/hw1-NLP/runs/yfpvutq0' target=\"_blank\">preds_translit_bl_emb_0.15_att_0.2res_0.2_relu_0.1_lr_0.002</a></strong> to <a href='https://wandb.ai/eksolodneva/hw1-NLP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/eksolodneva/hw1-NLP' target=\"_blank\">https://wandb.ai/eksolodneva/hw1-NLP</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/eksolodneva/hw1-NLP/runs/yfpvutq0' target=\"_blank\">https://wandb.ai/eksolodneva/hw1-NLP/runs/yfpvutq0</a>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n----------------------------------------\nEpoch: 1\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 31.75it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 3.815826 **\n\n----------------------------------------\nEpoch: 2\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.21it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 2.369842 **\n\n----------------------------------------\nEpoch: 3\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.10it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 1.197638 **\n\n----------------------------------------\nEpoch: 4\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.16it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.908341 **\n\n----------------------------------------\nEpoch: 5\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.86it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.744621 **\n\n----------------------------------------\nEpoch: 6\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.09it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.649059 **\n\n----------------------------------------\nEpoch: 7\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.43it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.598724 **\n\n----------------------------------------\nEpoch: 8\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.92it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.563207 **\n\n----------------------------------------\nEpoch: 9\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.12it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.555723 **\n\n----------------------------------------\nEpoch: 10\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.58it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.526684 **\n\n----------------------------------------\nEpoch: 11\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.07it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.505789 **\n\n----------------------------------------\nEpoch: 12\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.36it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.486882 **\n\n----------------------------------------\nEpoch: 13\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.79it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.478513 **\n\n----------------------------------------\nEpoch: 14\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.25it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.458887 **\n\n----------------------------------------\nEpoch: 15\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.17it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.453513 **\n\n----------------------------------------\nEpoch: 16\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.93it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.442731 **\n\n----------------------------------------\nEpoch: 17\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.47it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.435614 **\n\n----------------------------------------\nEpoch: 18\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.58it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.431412 **\n\n----------------------------------------\nEpoch: 19\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.38it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.423516 **\n\n----------------------------------------\nEpoch: 20\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.83it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.417221 **\nSaved checkpoint to bamblbi_fepoch_20\n\n----------------------------------------\nEpoch: 21\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.74it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.412255 **\n\n----------------------------------------\nEpoch: 22\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 32.96it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.410746 **\n\n----------------------------------------\nEpoch: 23\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.08it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.405322 **\n\n----------------------------------------\nEpoch: 24\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.37it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.402569 **\n\n----------------------------------------\nEpoch: 25\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.01it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.401167 **\n\n----------------------------------------\nEpoch: 26\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.78it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.396862 **\n\n----------------------------------------\nEpoch: 27\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.09it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.392966 **\n\n----------------------------------------\nEpoch: 28\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.38it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.388029 **\n\n----------------------------------------\nEpoch: 29\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.50it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.385230 **\n\n----------------------------------------\nEpoch: 30\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.26it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.383785 **\n\n----------------------------------------\nEpoch: 31\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.68it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.381893 **\n\n----------------------------------------\nEpoch: 32\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.32it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.379155 **\n\n----------------------------------------\nEpoch: 33\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.25it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.377888 **\n\n----------------------------------------\nEpoch: 34\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.66it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.374351 **\n\n----------------------------------------\nEpoch: 35\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.29it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.372308 **\n\n----------------------------------------\nEpoch: 36\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.55it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.370228 **\n\n----------------------------------------\nEpoch: 37\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.15it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.367873 **\n\n----------------------------------------\nEpoch: 38\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.51it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.366432 **\n\n----------------------------------------\nEpoch: 39\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.77it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.364590 **\n\n----------------------------------------\nEpoch: 40\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.40it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.363080 **\nSaved checkpoint to bamblbi_fepoch_40\n\n----------------------------------------\nEpoch: 41\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.66it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.360696 **\n\n----------------------------------------\nEpoch: 42\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.09it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.358651 **\n\n----------------------------------------\nEpoch: 43\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.11it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.358244 **\n\n----------------------------------------\nEpoch: 44\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.16it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.355134 **\n\n----------------------------------------\nEpoch: 45\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.08it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.354368 **\n\n----------------------------------------\nEpoch: 46\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.14it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.352433 **\n\n----------------------------------------\nEpoch: 47\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.02it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.350354 **\n\n----------------------------------------\nEpoch: 48\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.11it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.349979 **\n\n----------------------------------------\nEpoch: 49\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.58it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.347494 **\n\n----------------------------------------\nEpoch: 50\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.05it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.345837 **\n\n----------------------------------------\nEpoch: 51\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.22it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.344499 **\n\n----------------------------------------\nEpoch: 52\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.87it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.343815 **\n\n----------------------------------------\nEpoch: 53\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.01it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.341693 **\n\n----------------------------------------\nEpoch: 54\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.57it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.339189 **\n\n----------------------------------------\nEpoch: 55\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.68it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.338528 **\n\n----------------------------------------\nEpoch: 56\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.18it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.337955 **\n\n----------------------------------------\nEpoch: 57\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.45it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.335886 **\n\n----------------------------------------\nEpoch: 58\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.40it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.335103 **\n\n----------------------------------------\nEpoch: 59\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.30it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.333541 **\n\n----------------------------------------\nEpoch: 60\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.89it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.332388 **\nSaved checkpoint to bamblbi_fepoch_60\n\n----------------------------------------\nEpoch: 61\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.90it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.330945 **\n\n----------------------------------------\nEpoch: 62\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.71it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.330093 **\n\n----------------------------------------\nEpoch: 63\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.31it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.328253 **\n\n----------------------------------------\nEpoch: 64\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 32.95it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.327482 **\n\n----------------------------------------\nEpoch: 65\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.97it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.325332 **\n\n----------------------------------------\nEpoch: 66\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.25it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.325503 **\n\n----------------------------------------\nEpoch: 67\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.97it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.323579 **\n\n----------------------------------------\nEpoch: 68\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 32.97it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.322336 **\n\n----------------------------------------\nEpoch: 69\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.19it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.321286 **\n\n----------------------------------------\nEpoch: 70\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.12it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.320610 **\n\n----------------------------------------\nEpoch: 71\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.95it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.318713 **\n\n----------------------------------------\nEpoch: 72\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.00it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.317463 **\n\n----------------------------------------\nEpoch: 73\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.80it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.316273 **\n\n----------------------------------------\nEpoch: 74\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.24it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.315246 **\n\n----------------------------------------\nEpoch: 75\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.96it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.313930 **\n\n----------------------------------------\nEpoch: 76\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.00it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.311713 **\n\n----------------------------------------\nEpoch: 77\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.27it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.311391 **\n\n----------------------------------------\nEpoch: 78\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.85it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.311052 **\n\n----------------------------------------\nEpoch: 79\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.33it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.308834 **\n\n----------------------------------------\nEpoch: 80\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.25it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.308853 **\nSaved checkpoint to bamblbi_fepoch_80\n\n----------------------------------------\nEpoch: 81\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.81it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.307189 **\n\n----------------------------------------\nEpoch: 82\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.45it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.306036 **\n\n----------------------------------------\nEpoch: 83\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.73it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.305654 **\n\n----------------------------------------\nEpoch: 84\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.64it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.304746 **\n\n----------------------------------------\nEpoch: 85\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.37it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.303122 **\n\n----------------------------------------\nEpoch: 86\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.40it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.302359 **\n\n----------------------------------------\nEpoch: 87\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.35it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.302065 **\n\n----------------------------------------\nEpoch: 88\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.68it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.301258 **\n\n----------------------------------------\nEpoch: 89\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.66it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.299794 **\n\n----------------------------------------\nEpoch: 90\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.33it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.298708 **\n\n----------------------------------------\nEpoch: 91\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.27it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.298449 **\n\n----------------------------------------\nEpoch: 92\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.21it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.297340 **\n\n----------------------------------------\nEpoch: 93\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.42it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.295910 **\n\n----------------------------------------\nEpoch: 94\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.17it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.295425 **\n\n----------------------------------------\nEpoch: 95\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.32it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.294676 **\n\n----------------------------------------\nEpoch: 96\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.20it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.293617 **\n\n----------------------------------------\nEpoch: 97\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.21it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.292870 **\n\n----------------------------------------\nEpoch: 98\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.27it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.292338 **\n\n----------------------------------------\nEpoch: 99\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.10it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.291819 **\n\n----------------------------------------\nEpoch: 100\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.41it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.291544 **\nSaved checkpoint to bamblbi_fepoch_100\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:14<00:00,  9.16it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Using GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 10/10 [00:01<00:00,  9.12it/s]\n\u001b[32m[I 2023-04-11 20:12:26,475]\u001b[0m Trial 1 finished with value: 0.6393212360488953 and parameters: {'embedding': 0.15, 'attention': 0.2, 'residual': 0.2, 'relu': 0.1, 'lr_peak': 0.002}. Best is trial 1 with value: 0.6393212360488953.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Predictions saved to preds_translit_bl_f0.15_0.2_0.2_0.1_0.002.tsv\ndev set accuracy@1: 0.64\ndev_small set accuracy@1: 0.66\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Finishing last run (ID:yfpvutq0) before initializing another..."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='47.204 MB of 47.204 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, m…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "265b3b31dd72496b806ee71fb1db36b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss_train</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss_train</td><td>0.29154</td></tr></table><br/></div></div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">preds_translit_bl_emb_0.15_att_0.2res_0.2_relu_0.1_lr_0.002</strong> at: <a href='https://wandb.ai/eksolodneva/hw1-NLP/runs/yfpvutq0' target=\"_blank\">https://wandb.ai/eksolodneva/hw1-NLP/runs/yfpvutq0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 5 other file(s)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20230411_194523-yfpvutq0/logs</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Successfully finished last run (ID:yfpvutq0). Initializing new run:<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.14.2 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.14.0"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20230411_201227-0z2xtazm</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/eksolodneva/hw1-NLP/runs/0z2xtazm' target=\"_blank\">preds_translit_bl_emb_0.15_att_0.15res_0.15_relu_0.2_lr_0.001</a></strong> to <a href='https://wandb.ai/eksolodneva/hw1-NLP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/eksolodneva/hw1-NLP' target=\"_blank\">https://wandb.ai/eksolodneva/hw1-NLP</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/eksolodneva/hw1-NLP/runs/0z2xtazm' target=\"_blank\">https://wandb.ai/eksolodneva/hw1-NLP/runs/0z2xtazm</a>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n----------------------------------------\nEpoch: 1\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 31.86it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 4.080651 **\n\n----------------------------------------\nEpoch: 2\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.33it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 2.917995 **\n\n----------------------------------------\nEpoch: 3\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.08it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 1.929560 **\n\n----------------------------------------\nEpoch: 4\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.44it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 1.122450 **\n\n----------------------------------------\nEpoch: 5\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.85it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.899653 **\n\n----------------------------------------\nEpoch: 6\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.25it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.773904 **\n\n----------------------------------------\nEpoch: 7\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.33it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.685507 **\n\n----------------------------------------\nEpoch: 8\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.23it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.633948 **\n\n----------------------------------------\nEpoch: 9\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.43it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.591570 **\n\n----------------------------------------\nEpoch: 10\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.23it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.559699 **\n\n----------------------------------------\nEpoch: 11\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.22it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.525923 **\n\n----------------------------------------\nEpoch: 12\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.27it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.500096 **\n\n----------------------------------------\nEpoch: 13\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.70it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.480750 **\n\n----------------------------------------\nEpoch: 14\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.71it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.461272 **\n\n----------------------------------------\nEpoch: 15\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.52it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.449063 **\n\n----------------------------------------\nEpoch: 16\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.06it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.435489 **\n\n----------------------------------------\nEpoch: 17\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.40it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.427128 **\n\n----------------------------------------\nEpoch: 18\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.36it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.419301 **\n\n----------------------------------------\nEpoch: 19\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.93it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.410888 **\n\n----------------------------------------\nEpoch: 20\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.42it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.406228 **\nSaved checkpoint to bamblbi_fepoch_20\n\n----------------------------------------\nEpoch: 21\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.73it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.398207 **\n\n----------------------------------------\nEpoch: 22\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.33it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.395790 **\n\n----------------------------------------\nEpoch: 23\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.67it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.388935 **\n\n----------------------------------------\nEpoch: 24\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.05it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.383636 **\n\n----------------------------------------\nEpoch: 25\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.10it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.379067 **\n\n----------------------------------------\nEpoch: 26\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.90it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.376390 **\n\n----------------------------------------\nEpoch: 27\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.25it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.372751 **\n\n----------------------------------------\nEpoch: 28\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.24it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.369845 **\n\n----------------------------------------\nEpoch: 29\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.93it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.365342 **\n\n----------------------------------------\nEpoch: 30\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.30it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.364132 **\n\n----------------------------------------\nEpoch: 31\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.22it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.360340 **\n\n----------------------------------------\nEpoch: 32\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.20it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.357857 **\n\n----------------------------------------\nEpoch: 33\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.37it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.355829 **\n\n----------------------------------------\nEpoch: 34\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.14it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.352563 **\n\n----------------------------------------\nEpoch: 35\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.26it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.351477 **\n\n----------------------------------------\nEpoch: 36\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.23it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.348241 **\n\n----------------------------------------\nEpoch: 37\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.00it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.347049 **\n\n----------------------------------------\nEpoch: 38\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.29it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.345137 **\n\n----------------------------------------\nEpoch: 39\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.67it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.342394 **\n\n----------------------------------------\nEpoch: 40\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.85it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.341308 **\nSaved checkpoint to bamblbi_fepoch_40\n\n----------------------------------------\nEpoch: 41\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.00it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.339594 **\n\n----------------------------------------\nEpoch: 42\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.17it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.337609 **\n\n----------------------------------------\nEpoch: 43\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.63it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.336173 **\n\n----------------------------------------\nEpoch: 44\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.44it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.333449 **\n\n----------------------------------------\nEpoch: 45\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.65it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.333082 **\n\n----------------------------------------\nEpoch: 46\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.34it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.331455 **\n\n----------------------------------------\nEpoch: 47\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.06it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.330049 **\n\n----------------------------------------\nEpoch: 48\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.40it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.327556 **\n\n----------------------------------------\nEpoch: 49\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.60it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.325936 **\n\n----------------------------------------\nEpoch: 50\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.82it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.325643 **\n\n----------------------------------------\nEpoch: 51\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.60it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.324661 **\n\n----------------------------------------\nEpoch: 52\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.23it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.322460 **\n\n----------------------------------------\nEpoch: 53\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.21it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.321279 **\n\n----------------------------------------\nEpoch: 54\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.65it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.320153 **\n\n----------------------------------------\nEpoch: 55\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.17it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.318773 **\n\n----------------------------------------\nEpoch: 56\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.86it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.317585 **\n\n----------------------------------------\nEpoch: 57\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.70it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.317460 **\n\n----------------------------------------\nEpoch: 58\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.48it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.315807 **\n\n----------------------------------------\nEpoch: 59\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.21it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.314496 **\n\n----------------------------------------\nEpoch: 60\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.57it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.313317 **\nSaved checkpoint to bamblbi_fepoch_60\n\n----------------------------------------\nEpoch: 61\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.91it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.313101 **\n\n----------------------------------------\nEpoch: 62\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.43it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.312021 **\n\n----------------------------------------\nEpoch: 63\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.98it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.311018 **\n\n----------------------------------------\nEpoch: 64\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.53it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.308808 **\n\n----------------------------------------\nEpoch: 65\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.07it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.309149 **\n\n----------------------------------------\nEpoch: 66\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.11it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.307932 **\n\n----------------------------------------\nEpoch: 67\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.84it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.306218 **\n\n----------------------------------------\nEpoch: 68\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.47it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.305593 **\n\n----------------------------------------\nEpoch: 69\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.00it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.304434 **\n\n----------------------------------------\nEpoch: 70\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.09it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.303546 **\n\n----------------------------------------\nEpoch: 71\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.29it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.302318 **\n\n----------------------------------------\nEpoch: 72\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.61it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.302541 **\n\n----------------------------------------\nEpoch: 73\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.68it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.300626 **\n\n----------------------------------------\nEpoch: 74\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.20it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.299682 **\n\n----------------------------------------\nEpoch: 75\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.89it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.299370 **\n\n----------------------------------------\nEpoch: 76\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.18it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.298730 **\n\n----------------------------------------\nEpoch: 77\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.75it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.297545 **\n\n----------------------------------------\nEpoch: 78\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.82it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.297334 **\n\n----------------------------------------\nEpoch: 79\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.58it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.296387 **\n\n----------------------------------------\nEpoch: 80\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.14it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.295295 **\nSaved checkpoint to bamblbi_fepoch_80\n\n----------------------------------------\nEpoch: 81\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.27it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.295429 **\n\n----------------------------------------\nEpoch: 82\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.09it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.294001 **\n\n----------------------------------------\nEpoch: 83\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.33it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.293457 **\n\n----------------------------------------\nEpoch: 84\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.29it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.292055 **\n\n----------------------------------------\nEpoch: 85\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.72it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.292159 **\n\n----------------------------------------\nEpoch: 86\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.82it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.291041 **\n\n----------------------------------------\nEpoch: 87\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.58it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.290949 **\n\n----------------------------------------\nEpoch: 88\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.60it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.290250 **\n\n----------------------------------------\nEpoch: 89\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.76it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.289624 **\n\n----------------------------------------\nEpoch: 90\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.83it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.288499 **\n\n----------------------------------------\nEpoch: 91\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.92it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.288202 **\n\n----------------------------------------\nEpoch: 92\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.13it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.287305 **\n\n----------------------------------------\nEpoch: 93\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.09it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.286813 **\n\n----------------------------------------\nEpoch: 94\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.56it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.286695 **\n\n----------------------------------------\nEpoch: 95\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.83it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.286399 **\n\n----------------------------------------\nEpoch: 96\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.88it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.285481 **\n\n----------------------------------------\nEpoch: 97\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.30it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.284186 **\n\n----------------------------------------\nEpoch: 98\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.70it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.284236 **\n\n----------------------------------------\nEpoch: 99\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.52it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.284176 **\n\n----------------------------------------\nEpoch: 100\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.04it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.284043 **\nSaved checkpoint to bamblbi_fepoch_100\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:14<00:00,  9.02it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Using GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 10/10 [00:01<00:00,  9.32it/s]\n\u001b[32m[I 2023-04-11 20:39:24,583]\u001b[0m Trial 2 finished with value: 0.6439526231873054 and parameters: {'embedding': 0.15, 'attention': 0.15, 'residual': 0.15, 'relu': 0.2, 'lr_peak': 0.001}. Best is trial 2 with value: 0.6439526231873054.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Predictions saved to preds_translit_bl_f0.15_0.15_0.15_0.2_0.001.tsv\ndev set accuracy@1: 0.64\ndev_small set accuracy@1: 0.66\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Finishing last run (ID:0z2xtazm) before initializing another..."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='47.204 MB of 47.204 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, m…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db657f730dff42718e34ad780e360617"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss_train</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss_train</td><td>0.28404</td></tr></table><br/></div></div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">preds_translit_bl_emb_0.15_att_0.15res_0.15_relu_0.2_lr_0.001</strong> at: <a href='https://wandb.ai/eksolodneva/hw1-NLP/runs/0z2xtazm' target=\"_blank\">https://wandb.ai/eksolodneva/hw1-NLP/runs/0z2xtazm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 5 other file(s)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20230411_201227-0z2xtazm/logs</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Successfully finished last run (ID:0z2xtazm). Initializing new run:<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668858916667282, max=1.0…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dceec8dd60c7434291da1914f19073c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.14.2 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.14.0"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20230411_203925-28eehuh2</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/eksolodneva/hw1-NLP/runs/28eehuh2' target=\"_blank\">preds_translit_bl_emb_0.1_att_0.15res_0.2_relu_0.1_lr_0.001</a></strong> to <a href='https://wandb.ai/eksolodneva/hw1-NLP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/eksolodneva/hw1-NLP' target=\"_blank\">https://wandb.ai/eksolodneva/hw1-NLP</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/eksolodneva/hw1-NLP/runs/28eehuh2' target=\"_blank\">https://wandb.ai/eksolodneva/hw1-NLP/runs/28eehuh2</a>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n----------------------------------------\nEpoch: 1\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:17, 30.86it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 4.140375 **\n\n----------------------------------------\nEpoch: 2\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.19it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 2.905445 **\n\n----------------------------------------\nEpoch: 3\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.44it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 1.777207 **\n\n----------------------------------------\nEpoch: 4\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.26it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 1.051733 **\n\n----------------------------------------\nEpoch: 5\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 32.97it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.846230 **\n\n----------------------------------------\nEpoch: 6\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.36it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.704747 **\n\n----------------------------------------\nEpoch: 7\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.16it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.625566 **\n\n----------------------------------------\nEpoch: 8\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.42it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.577045 **\n\n----------------------------------------\nEpoch: 9\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.53it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.541610 **\n\n----------------------------------------\nEpoch: 10\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.86it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.515631 **\n\n----------------------------------------\nEpoch: 11\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.25it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.485034 **\n\n----------------------------------------\nEpoch: 12\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.62it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.463827 **\n\n----------------------------------------\nEpoch: 13\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.05it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.444624 **\n\n----------------------------------------\nEpoch: 14\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.37it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.429483 **\n\n----------------------------------------\nEpoch: 15\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.86it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.416389 **\n\n----------------------------------------\nEpoch: 16\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.61it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.407497 **\n\n----------------------------------------\nEpoch: 17\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.60it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.399744 **\n\n----------------------------------------\nEpoch: 18\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.88it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.392562 **\n\n----------------------------------------\nEpoch: 19\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.28it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.384226 **\n\n----------------------------------------\nEpoch: 20\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.53it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.379708 **\nSaved checkpoint to bamblbi_fepoch_20\n\n----------------------------------------\nEpoch: 21\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.05it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.373595 **\n\n----------------------------------------\nEpoch: 22\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.34it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.370009 **\n\n----------------------------------------\nEpoch: 23\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.08it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.365998 **\n\n----------------------------------------\nEpoch: 24\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.63it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.361224 **\n\n----------------------------------------\nEpoch: 25\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.70it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.356718 **\n\n----------------------------------------\nEpoch: 26\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.31it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.353296 **\n\n----------------------------------------\nEpoch: 27\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.30it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.351587 **\n\n----------------------------------------\nEpoch: 28\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.32it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.348763 **\n\n----------------------------------------\nEpoch: 29\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.32it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.346149 **\n\n----------------------------------------\nEpoch: 30\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.05it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.343021 **\n\n----------------------------------------\nEpoch: 31\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.06it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.340358 **\n\n----------------------------------------\nEpoch: 32\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.49it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.337245 **\n\n----------------------------------------\nEpoch: 33\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.35it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.335790 **\n\n----------------------------------------\nEpoch: 34\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.56it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.332585 **\n\n----------------------------------------\nEpoch: 35\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.35it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.331654 **\n\n----------------------------------------\nEpoch: 36\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.14it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.329549 **\n\n----------------------------------------\nEpoch: 37\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.20it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.327364 **\n\n----------------------------------------\nEpoch: 38\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.47it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.325509 **\n\n----------------------------------------\nEpoch: 39\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.33it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.325070 **\n\n----------------------------------------\nEpoch: 40\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.43it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.321986 **\nSaved checkpoint to bamblbi_fepoch_40\n\n----------------------------------------\nEpoch: 41\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.62it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.320142 **\n\n----------------------------------------\nEpoch: 42\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.57it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.319101 **\n\n----------------------------------------\nEpoch: 43\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.60it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.316842 **\n\n----------------------------------------\nEpoch: 44\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.06it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.316264 **\n\n----------------------------------------\nEpoch: 45\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.50it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.314388 **\n\n----------------------------------------\nEpoch: 46\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.00it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.313560 **\n\n----------------------------------------\nEpoch: 47\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.60it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.311800 **\n\n----------------------------------------\nEpoch: 48\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.65it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.310555 **\n\n----------------------------------------\nEpoch: 49\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.86it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.308885 **\n\n----------------------------------------\nEpoch: 50\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.78it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.307983 **\n\n----------------------------------------\nEpoch: 51\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.90it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.306239 **\n\n----------------------------------------\nEpoch: 52\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.50it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.306094 **\n\n----------------------------------------\nEpoch: 53\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.49it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.304210 **\n\n----------------------------------------\nEpoch: 54\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.14it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.302951 **\n\n----------------------------------------\nEpoch: 55\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.61it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.302405 **\n\n----------------------------------------\nEpoch: 56\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.24it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.300857 **\n\n----------------------------------------\nEpoch: 57\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.76it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.300032 **\n\n----------------------------------------\nEpoch: 58\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.71it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.298985 **\n\n----------------------------------------\nEpoch: 59\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.03it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.297774 **\n\n----------------------------------------\nEpoch: 60\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 34.34it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.297395 **\nSaved checkpoint to bamblbi_fepoch_60\n\n----------------------------------------\nEpoch: 61\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.93it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.295727 **\n\n----------------------------------------\nEpoch: 62\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.97it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.294818 **\n\n----------------------------------------\nEpoch: 63\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "244it [00:07, 34.89it/s]",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FiOMwkAVG82W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Label smoothing"
      ],
      "metadata": {
        "id": "7hMYmIO2tf8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We suggest to implement an additional regularization method - **label smoothing**. Now imagine that we have a prediction vector from probabilities at position t in the sequence of tokens for each token id from the vocabulary. CrossEntropy compares it with ground truth one-hot representation\n",
        "\n",
        "$$[0, ... 0, 1, 0, ..., 0].$$\n",
        "\n",
        "And now imagine that we are slightly \"smoothed\" the values in the ground truth vector and obtained\n",
        "\n",
        "$$[\\frac{\\alpha}{|V|}, ..., \\frac{\\alpha}{|V|}, 1(1-\\alpha)+\\frac{\\alpha}{|V|},  \\frac{\\alpha}{|V|}, ... \\frac{\\alpha}{|V|}],$$\n",
        "\n",
        "where $\\alpha$ - parameter from 0 to 1, $|V|$ - vocabulary size - number of components in the ground truth vector. The values ​​of this new vector are still summed to 1. Calculate the cross-entropy of our prediction vector and the new ground truth. Now, firstly, cross-entropy will never reach 0, and secondly, the result of the error function will require the model, as usual, to return the highest probability vector compared to other components of the probability vector for the correct token in the dictionary, but at the same time not too large, because as the value of this probability approaches 1, the value of the error function increases. For research on the use of label smoothing, see the [paper](https://arxiv.org/abs/1906.02629).\n",
        "    \n",
        "Accordingly, in order to embed label smoothing into the model, it is necessary to carry out the transformation described above on the ground truth vectors, as well as to implement the cross-entropy calculation, since the used `torch.nn.CrossEntropy` class is not quite suitable, since for the ground truth representation of `__call__` method takes the id of the correct token and builds a one-hot vector already inside. However, it is possible to implement what is required based on the internal implementation of this class [CrossEntropyLoss](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#CrossEntropyLoss).\n",
        "    \n",
        "\n",
        "Test different values of $\\alpha$ (e.x, 0.05, 0.1, 0.2). Describe your experiments and results.\n"
      ],
      "metadata": {
        "id": "setYzbjCtqZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelSmoothing(nn.Module):\n",
        "    \"Implement label smoothing.\"\n",
        "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
        "        super(LabelSmoothing, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(size_average=False)\n",
        "        self.padding_idx = padding_idx\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.size = size\n",
        "        self.true_dist = None\n",
        "        \n",
        "    def forward(self, x, target):\n",
        "        assert x.size(1) == self.size\n",
        "        true_dist = x.data.clone()\n",
        "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
        "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        true_dist[:, self.padding_idx] = 0\n",
        "        mask = torch.nonzero(target.data == self.padding_idx)\n",
        "        if mask.dim() > 0:\n",
        "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "        self.true_dist = true_dist\n",
        "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
      ],
      "metadata": {
        "id": "9ZU-88SKPj36",
        "execution": {
          "iopub.status.busy": "2023-04-11T15:32:28.793135Z",
          "iopub.execute_input": "2023-04-11T15:32:28.793551Z",
          "iopub.status.idle": "2023-04-11T15:32:28.811855Z",
          "shell.execute_reply.started": "2023-04-11T15:32:28.793505Z",
          "shell.execute_reply": "2023-04-11T15:32:28.807774Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rpu2txn-G82Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}